\documentclass[11pt,a4paper]{article}
\usepackage{bm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{physics}
\usepackage[margin=1.25in]{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{color}
\usepackage{amsmath}
\numberwithin{equation}{section}
\linespread{1.3}
\DeclareMathOperator{\Vect}{vec}

\let\OLDthebibliography\thebibliography
\renewcommand\thebibliography[1]{
	\OLDthebibliography{#1}
	\setlength{\parskip}{0pt}
	\setlength{\itemsep}{0pt plus 0.3ex}
}

\begin{document}
	\pagenumbering{arabic}
	\begin{titlepage}
		\begin{center}
			\vspace*{1cm}
			
			\textbf{\LARGE{Hamiltonian Control of Open Gaussian Systems}}
			
			\vspace{0.5cm}
			
			\large{Rianna Kelly}
			
			\vfill
			
			A thesis presented for...\\
			...
			
			\vspace{0.8cm}
			
			Department of Physics and Astronomy\\
			University College London\\
			August 2017
			
			\vspace{0.5\textheight}
			
		\end{center}
	\end{titlepage}

	\color{red}\begin{abstract}
	Lorem ipsum dolor sit amet, consectetur adipiscing elit. Quisque lacus sem, ornare in commodo venenatis, pellentesque eget eros. Praesent at laoreet leo. Aliquam vitae elit eu tortor malesuada luctus at non lectus. Ut dapibus quis est nec facilisis. Phasellus et vehicula tellus. Suspendisse ut fringilla tortor. Aenean convallis hendrerit maximus. Ut aliquet purus in eros tincidunt, aliquet semper quam gravida. Donec nec augue quis eros malesuada scelerisque et non neque. Curabitur et orci consequat, vehicula neque vitae, facilisis mauris. Aliquam nec interdum dui. Nam fermentum sem lorem, eu condimentum ex bibendum a. Aliquam eu scelerisque magna, vitae feugiat orci. Nunc.
	\end{abstract}\color{black}
	
	\tableofcontents
	\listoffigures

	\newpage
		
	\color{red}\section{Introduction}
	Lorem ipsum dolor sit amet, consectetur adipiscing elit. Quisque lacus sem, ornare in commodo venenatis, pellentesque eget eros. Praesent at laoreet leo. Aliquam vitae elit eu tortor malesuada luctus at non lectus. Ut dapibus quis est nec facilisis. Phasellus et vehicula tellus. Suspendisse ut fringilla tortor. Aenean convallis hendrerit maximus. Ut aliquet purus in eros tincidunt, aliquet semper quam gravida. Donec nec augue quis eros malesuada scelerisque et non neque. Curabitur et orci consequat, vehicula neque vitae, facilisis mauris. Aliquam nec interdum dui. Nam fermentum sem lorem, eu condimentum ex bibendum a. Aliquam eu scelerisque magna, vitae feugiat orci. Nunc.
	
	Lorem ipsum dolor sit amet, consectetur adipiscing elit. Quisque lacus sem, ornare in commodo venenatis, pellentesque eget eros. Praesent at laoreet leo. Aliquam vitae elit eu tortor malesuada luctus at non lectus. Ut dapibus quis est nec facilisis. Phasellus et vehicula tellus. Suspendisse ut fringilla tortor. Aenean convallis hendrerit maximus. Ut aliquet purus in eros tincidunt, aliquet semper quam gravida. Donec nec augue quis eros malesuada scelerisque et non neque. Curabitur et orci consequat, vehicula neque vitae, facilisis mauris. Aliquam nec interdum dui. Nam fermentum sem lorem, eu condimentum ex bibendum a. Aliquam eu scelerisque magna, vitae feugiat orci. Nunc.\color{black}
	
	
	\subsection{The Density Operator}
	\label{sec:densityop}
	We begin by summarising the Dirac formalism of quantum mechanics and motivating the need for a density operator approach to open systems. It is a fundamental postulate of quantum mechanics that the state vector, $\ket{\psi}$, provides a complete physical description of the system.  To any physical system, we associate a Hilbert space (the `state space'), which represents the entire set of states that the state vector can take. Operators are taken to act on elements of the Hilbert space, mapping each vector to some other vector within the space. Of particular importance are the `observable operators' which make precise our notion of measurement; every physically observable quantity has, associated with it, a linear Hermitian operator whose (real) eigenvalues represent possible outcomes of a measurement \cite{vonNeumann55}. We refer to `possible outcomes' to emphasise that, at its core, quantum mechanics is inherently probabilistic and measurements are typically described in terms of their expectation values.
	
	Of course, in the real world, it is rare to have complete knowledge of a system. Alongside quantum mechanical uncertainty, there may also be classical uncertainties relating to what state the system is actually assumed to be in. For instance, uncertainties in an experimental setup could be known to generate any of $n$ possible states, $\ket{\psi_n}$, each with a probability $p_n$. Our expectation of any measurement would then be a function of two unknowns: the inherent quantum mechanical uncertainty associated with any measurement of the state $\ket{\psi_n}$, as well as the classical uncertainty relating to which of the $n$ possible states the system is in at the moment of measurement. This `statistical mixture' of unknowns can be completely captured by introducing the density operator, $\hat{\rho}$, which we define \cite{Fano57}
	
	\begin{equation} \label{eq:1}
	\hat{\rho} = \sum_{j=1}^{n} p_j \ket{\psi_j}\bra{\psi_j}.
	\end{equation}
	
	The density operator represents our best description of the system and can, for all practical purposes, be regarded as the state of the system (in place of the state vector $\ket{\psi}$). To avoid confusion, we note at the offset that the density operator can, when equipped with a basis that is discrete and finite, be represented by a matrix and we shall use the terms density matrix and density operator interchangeably.
	
	From our definition \ref{eq:1}, it follows that the density matrix is both positive semi-definite, 
	\begin{equation} \label{eq:2}
	\hat{\rho} \geq 0,
	\end{equation}
	and Hermitian,
	\begin{equation} \label{eq:3}
	\hat{\rho} = \hat{\rho}^\dagger .
	\end{equation}
	It can also be shown to have unit trace, 
	\begin{equation} \label{eq:4}
	\Tr[\hat{\rho}] = 1.
	\end{equation}
	
	As stated above, the main drive behind using a density operator approach stems from our desire to completely describe systems, accounting for all sources of uncertainty. It can be shown that the expectation value of any observable operator, $\hat{O}$, can be written as
	\begin{equation} \label{eq:5}
	\Tr[\hat{O}\hat{\rho}].
	\end{equation}
	
	This equation neatly captures all of the measurable/observable uncertainties of the system and, therefore, justifies our usage of density operators.
	
	\subsection{Discrete Variable Systems}
	\color{red}AS says to remove this section \color{black}Our focus in this report is on CV systems. Before discussing those, however, it is worth briefly noting that discrete variable systems exist and arise frequently in quantum mechanics. As the name suggests, discrete variable systems are ones in which the possible outcomes of a measurement are discrete (or quantised). This manifests itself mathematically via operators whose eigenvalues are discrete. If, in addition to being discrete, the eigenvalues form a finite set, the system can be handled using matrix vector methods. There are numerous examples of discrete variable systems such as \color{red} the energy levels of a quantum harmonic oscillator (infinite dimensional Hilbert space) or \color{black}the spin of an electron (two dimensional Hilbert space).
	
	\section{Continuous Variable Systems}
	\label{sec:cvs}
	Eigenvalues (observable values) of a system can also have a continuous spectra which naturally implies that the associated Hilbert space is infinite dimensional. Perhaps, the most obvious examples are the position or momentum of a particle. Noting that any measurement of, say, position must be an eigenvalue of the position operator it must follow that the state space is infinite in extent. 
	
	CV systems are quantum systems described by a pair of canonical operators, i.e. the pair are related via a Fourier transform. Position and momentum are one such example of a canonical/conjugate pair. Canonical pairs satisfy the canonical commutation relation which, for position and momentum, can be written \cite{Serafini05}
	\begin{equation} \label{eq:6}
	[\hat{x}, \hat{p}] = i\hbar\hat{\mathbb{I}}.
	\end{equation}
	
	One of the most fundamental examples of a CV system is the quantum harmonic oscillator which describes the dynamics of a particle in a quadratic potential. The Hamiltonian can be written as \cite{Adesso14, Braunstein}
	\begin{equation} \label{eq:7}
	\hat{H} = \frac{\hat{p}^2}{2m} + \frac{1}{2} m\omega^2\hat{x}^2,
	\end{equation}
	
	where $m$ is taken to be the mass of the particle and $\omega$, the angular frequency. For a system with $N$ non-interacting modes, the Hamiltonian can be written as a sum of the individual Hamiltonians 
	
	\begin{equation} \label{eq:8}
	\hat{H} = \sum_{k=1}^{N}\hat{H_k},\qquad\hat{H_k} =\frac{\hat{p_k}^2}{2m} + \frac{1}{2} m\omega_{k}^2\hat{x_k}^2.
	\end{equation}
	Here, $\hat{x_k}$ and $\hat{p_k}$ are canonical pairs satisfying the canonical commutation relation 
	\begin{equation} \label{eq:9}
	[\hat{x_j}, \hat{p_k}] = i\hbar \delta_{j\,k}\hat{\mathbb{I}} .
	\end{equation}
	It is convenient to group these operators into a single vector defined
	\begin{equation} \label{eq:10}
	\mathbf{\hat{r}} = (\hat{x_1},\hat{p_1},\hat{x_2},\hat{p_2},...,\hat{x_N},\hat{p_N})^T = (\hat{r}_1, \hat{r}_2, ..., \hat{r}_{2N})^T .
	\end{equation}
	From which the following commutation relations can be derived (using Eq.\ref{eq:6})
	\begin{equation*}
	[\hat{r}_1, \hat{r}_1] = [\hat{x_1}, \hat{x_2}] = 0,
	\end{equation*} 
	\begin{equation*}
	[\hat{r}_1, \hat{r}_2] = [\hat{x_1}, \hat{p_1}] = i,
	\end{equation*} 
	\begin{equation*}
	[\hat{r}_2, \hat{r}_1] = [\hat{p_1}, \hat{x_1}] = -i,
	\end{equation*} 
	\begin{equation*}
	[\hat{r}_2, \hat{r}_2] = [\hat{p_1}, \hat{p_2}] = 0,
	\end{equation*}
	which gives the matrix 
	\begin{equation*}
	\begin{pmatrix}
	0 & i  \\
	-i & 0 \\
	\end{pmatrix}.
	\end{equation*}
	Then the canonical commutation relation can be written as
	\begin{equation} \label{eq:11}
	[\hat{r_j}, \hat{r_k}] = i\Omega_{jk}\mathbb{\hat{I}},
	\end{equation}
	where $\Omega$ is a $2N \times 2N$, anti-symmetric matrix known as the symplectic form. It can be written as a matrix direct sum as follows
	\begin{equation} \label{eq:12}
	\Omega = \bigoplus_{j=1}^{N}\omega, \qquad\omega = 
	\begin{pmatrix}
	0 & 1  \\
	-1 & 0 \\
	\end{pmatrix}.
	\end{equation}
	
	The symplectic form is of considerable importance within CV systems, specifically within a set of states known as Gaussian states. The symplectic framework and Gaussian states are discussed in detail in \ref{sec:gaussian}. 
	
	\subsection{Phase Space Representation}
	
	As noted in Section \ref{sec:densityop}, the state of a quantum system can be represented using a density operator approach. Additionally, we can use $\hat{\rho}$ to build a phase space representation of the system where a real distribution is defined over the $2N$ dimensions in that space. 
	
	To transition between the representations, we must first introduce the  Weyl displacement operator, $\hat{D}_\textbf{r}$, \cite{Olivares12}
	\begin{equation} \label{eq:13}
	\hat{D}_\mathbf{r} = e^{i\mathbf{r}^{T}\Omega\mathbf{\hat{r}}}, 
	\end{equation}
	
	where $\mathbf{r} = (x_1, p_1, ..., x_N, p_N)^T $ represents a point in phase space and $\mathbf{\hat{r}}$ is the vector of canonical operators already introduced. As the name suggests, the displacement operator, $\hat{D}_\mathbf{r}$, shifts the state of the system by an amount $\mathbf{r}$ in phase space. The action of this is shown in Figure \ref{fig:RKPOINT}. 
	
	It can be shown that the set of all displacement operators, $\hat{D}_\mathbf{r}$, for all $\mathbf{r}$ within the phase space, form a basis in operator space, i.e. any operator can be represented as a linear combination of displacement operators. Given $\mathbf{r}$ varies continuously within the phase space, it is not surprising that the linear combination is represented via an integral; an arbitrary operator, $\hat{O}$, can be represented as a Fourier-Weyl transform \cite{Cahill68}
	\begin{equation} \label{eq:14}
	\hat{O} = \frac{1}{(2\pi)^n}\int \chi(\mathbf{r})\hat{D}_\textbf{r}  d\textbf{r} = \frac{1}{(2\pi)^n} \int \Tr[\hat{D}_\textbf{r}^\dagger\hat{O}]\hat{D}_\textbf{r}  d\textbf{r}, \qquad \hat{D}_\mathbf{r}^\dagger  = \hat{D}_\mathbf{-r}.
	\end{equation}  
	
	Here, $\chi(\mathbf{r})$ can be interpreted as a function that modulates the amplitude of $\hat{D}_\textbf{r}$. This equally well applies to an arbitrary quantum state, which can be written using a density operator
	\begin{equation} \label{eq:15}
	\hat{\rho} = \frac{1}{(2\pi)^n} \int \chi_\rho(\mathbf{r})\hat{D}_\textbf{r}  d\textbf{r} = \frac{1}{(2\pi)^n} \int \Tr[\hat{D}_\textbf{r}^\dagger \hat{\rho}]\hat{D}_\textbf{r}  d\textbf{r}.
	\end{equation} 
	
	\begin{figure}[h]
		\centering
		\includegraphics[scale = 1]{RKPOINT}
		\caption[Displacement operator]{The displacement operator, $\hat{D}_\mathbf{r}$, shifts the state of the system in phase space.}
		\label{fig:RKPOINT}
	\end{figure}
	
	The action of this is shown in Figure \ref{fig:RKDENSITY}. We refer to $\chi_\rho$ as the characteristic function and, for every quantum state, $\hat{\rho}$, a corresponding characteristic function, $\chi_\rho$, exists \cite{Hillery84}. The characteristic function of the density operator is crucial to the definition of a Gaussian state, which is a key focus of this paper.
	
	\begin{figure}[h]
		\centering
		\includegraphics[scale = 1]{RKDENSITY}
		\caption[Think of name]{The state of the system can be represented as a linear combination of displacement operators whose amplitude is modulated by the characteristic function.}
		\label{fig:RKDENSITY}
	\end{figure}
	
	
	\section{Gaussian States}
	\label{sec:gaussian} 
	\subsection{Definition and Interpretation}
	
	A Gaussian state is one in which the characteristic function is Gaussian. The utility of such a description comes about from the relatively few parameters required to characterise this state. A one dimension Gaussian distribution is completely characterised by its first and second moments (the mean and variance) \cite{Ribeiro}. The generalisation to a $2N$ dimensional phase space is straightforward; the Gaussian state is completely characterised by a $2N$ dimensional vector of means and a $2N\times2N$ covariance matrix. The use of Gaussian states makes the handling of CV systems significantly more mathematically manageable.
	
	For $x \in \mathbb{R}^{2N}$, the standard Gaussian function can be written as \cite{Adesso14}
	\begin{equation} \label{eq:16}
	f(x) = Ae^{-\frac{1}{2}x^{T}Bx+ x^{T} y},
	\end{equation}
	where B is a $2N \times 2N$ matrix and $y$ is the mean vector. A quantum state $\hat{\rho}$ is Gaussian if and only if its characteristic function $\chi_\rho$ is a Gaussian function in phase space \cite{Schumaker86}, that is to say $\chi_\rho$ is of the form
	\begin{equation} \label{eq:17}
	\chi_G(\mathbf{r}) = e^{-\frac{1}{4}(\Omega^T\mathbf{r})^T\sigma(\Omega^T\mathbf{r}) - i(\Omega^T\mathbf{r})^T\mathbf{\bar{r}}},
	\end{equation} where $\sigma$ is taken to be the covariance matrix and $\mathbf{\bar{r}}$ is the vector of first moments. Written explicitly \cite{Adesso14},
	\begin{equation} \label{eq:18}
	\mathbf{\bar{r}} = (\langle\hat{x_1}\rangle, \langle\hat{p_1}\rangle, \langle\hat{x_2}\rangle, \langle\hat{p_2}\rangle, ..., \langle\hat{x_N}\rangle, \langle\hat{p_N}\rangle) = \Tr[\hat{\rho} \mathbf{\hat{r}}] ,
	\end{equation}
	and the covariance matrix has components,
	\begin{equation} \label{eq:19}
	\sigma_{jk} = \langle \mathbf{\hat{r}}_j\mathbf{\hat{r}}_k + \mathbf{\hat{r}}_k\mathbf{\hat{r}}_j \rangle - 2\langle \mathbf{\hat{r}}_j \rangle \langle \mathbf{\hat{r}}_k \rangle = \Tr[\hat{\rho} \{\mathbf{\hat{r_j}} - \mathbf{\bar{r}_j}, \mathbf{\hat{r}_k} - \mathbf{\bar{r}_k}\}].
	\end{equation}
	
	
	The covariance matrix is of central importance as it contains all the necessary information regarding properties such as purity and entanglement. It is an active area of research for many \cite{Adesso07, Plenio03}, and we often choose to isolate the matrix by taking the first moments to be zero ($\mathbf{\bar{r}} = 0$).
	
	Equivalently, Gaussian states can be interpreted as the ground and thermal states of systems with quadratic Hamiltonians \cite{Genoni16}. A quadratic Hamiltonian is defined as one whose position and momenta operators are, at most, quadratic in its definition. The $N$-mode quantum harmonic oscillator is an obvious example where both the momentum and position operators appear quadratically. They can thus be represented in matrix form as follows
	\begin{equation} \label{eq:20}
	\hat{H} = \frac{1}{2}\mathbf{\hat{r}}^{T}H\mathbf{\hat{r}}+\mathbf{\hat{r}}^{T}\mathbf{a}.
	\end{equation}
	
	
	A state $\hat{\rho}$ is Gaussian if and only if the Hamiltonian matrix $ \mathit{H} > 0$. Physically, this is reasonable as we are only considering stable systems (there must be a stable lower bound on $H$) and, mathematically, $H$ must be positive definite as $\sigma>0$. Therefore, we can equally well define a Gaussian state
	\begin{equation} \label{eq:21}
	\hat{\rho}_G = \frac{e^{-\beta \hat{H}}}{\Tr[e^{-\beta \hat{H}}]}.
	\end{equation}\\
	where $\beta = \frac{1}{k_BT}$ and the denominator is included to ensure that $\hat{\rho}_G$ has unit trace as per Eq.\ref{eq:4}. Assuming the linear terms do not contribute significantly to the quadratic Hamiltonian, $\hat{H}$, we can again isolate the quadratic matrix by dropping the linear terms completely \cite{Genoni16}, i.e.
	\begin{equation} \label{eq:22}
	\hat{H} = \frac{1}{2}\mathbf{\hat{r}}^{T}H\mathbf{\hat{r}}.
	\end{equation}
	
	Given the simplicity of this approach, Gaussian states hold a prominent position within many areas of research including optomechanics \cite{Genoni15} and atomic ensembles \cite{Sherson}. There have also been a number of reviews outlining the central role of Gaussian states within CV quantum information processing \cite{Napoli05, Weedbrook12}. 
	
	\subsection{Symplectic Framework}
	\label{sec:symplectic}
	Gaussian states are associated with the mathematical framework of the real symplectic group, $Sp(2N, \mathbb{R})$ \cite{Arvind95}. This group is characterised by the properties of symplectic transformations. A symplectic transformation is a unitary which maps a Gaussian state to another Gaussian state within the phase space. The (linear) symplectic transformation maps the first moments according to
	\begin{equation}\label{eq:23}
	\mathbf{\bar{r}}' = S \mathbf{\bar{r}},
	\end{equation}
	and the second moments
	\begin{equation}\label{eq:24}
	\sigma' = S\sigma S^T,
	\end{equation}
	where S is a real, symplectic matrix \cite{Adesso14}.
	
	The set of all real symplectic matrices, $\{S\}$, then form a group within the real, $2N$ dimensional phase space called the symplectic group, $Sp(2N, \mathbb{R})$, that preserve the symplectic form $\Omega$ defined in Eq.\ref{eq:12}
	\begin{equation} \label{eq:25}
	\Omega = S\Omega S^T .
	\end{equation}
	
	For example, a squeezing transformation $S_r$, for $r \in \mathbb{R}$, is given as
	\begin{equation*}
	S_r = \begin{pmatrix}
	e^r & 0  \\
	0 & e^{-r} \\
	\end{pmatrix}.
	\end{equation*}
	
	Then, using Eq.\ref{eq:25} we can see the symplectic form is preserved
	\begin{equation*}
	S_r\Omega S_r^{T} = \begin{pmatrix}
	e^r & 0  \\
	0 & e^{-r} \\
	\end{pmatrix}
	\begin{pmatrix}
	0 & 1  \\
	-1 & 0 \\
	\end{pmatrix}
	\begin{pmatrix}
	e^r & 0  \\
	0 & e^{-r} \\
	\end{pmatrix}  = \begin{pmatrix}
	0 & e^re^{-r}  \\
	-e^{-r}e^r & 0 \\
	\end{pmatrix} = \begin{pmatrix}
	0 & 1 \\
	-1 & 0 \\
	\end{pmatrix} = \Omega .
	\end{equation*}
	
	Williamson's Theorem \cite{Williamson36} states that for every real, symmetric, positive-definite matrix  $\sigma$ there exists a symplectic transformation $S \in Sp(2N, \mathbb{R})$ that diagonalises $\sigma$
	\begin{equation} \label{eq:27}
	S\sigma S^T = \nu,
	\end{equation}
	where $\nu$ are the symplectic eigenvalues of $\sigma$ 
	\begin{equation} \label{eq:28}
	\nu = \bigoplus_{j=1}^{N} 
	\begin{pmatrix}
	\nu_j & 0  \\
	0 & \nu_j \\
	\end{pmatrix}.
	\end{equation}
	
	The set of symplectic eigenvalues of $\sigma$ form the symplectic spectrum, $\{\nu_j\}$. These symplectic eigenvalues can also be calculated by considering the (standard, non-symplectic) eigenvalues of $\lvert i\Omega \sigma \lvert$. Furthermore, using the Robertson-Schr{\"o}dinger uncertainty principle outlined in Section \ref{sec:uncertainty} it is possible to show that $\nu_j\geq 1$ for all $j = 1, ..., N$ \cite{Serafini05}.
	
	\subsection{The Uncertainty Principle}
	\label{sec:uncertainty}
	The covariance matrix, $\sigma$, is the covariance matrix of a quantum state if and only if it satisfies the uncertainty principle \cite{Simon94}. The Robertson-Schr{\"o}dinger uncertainty principle is given as \cite{Rob29, Sch30}
	\begin{equation} \label{eq:29}
	\sigma + i\Omega \geq 0.
	\end{equation} 
	We can show that the familiar Heisenberg uncertainty principle is a special case of the Robertson-Schr{\"o}dinger uncertainty principle. For $\mathbf{\hat{r}} = (\hat{x}, \hat{p})^T$, the covariance matrix is given as
	\begin{equation*}
	\sigma = 
	\begin{pmatrix}
	\sigma_{xx} & \sigma_{xp}  \\
	\sigma_{px} & \sigma_{pp} \\
	\end{pmatrix}
	= 
	\begin{pmatrix}
	2\Delta\hat{x}^2 & 0  \\
	0 & 2\Delta\hat{p}^2 \\
	\end{pmatrix} \qquad\text{where we assume $\sigma_{xp}=\sigma_{px}= \langle \hat{x}\hat{p} + \hat{p}\hat{x} \rangle - 2\bar{x}\bar{p} = 0$}.
	\end{equation*}
	From Eqs.\ref{eq:12} and \ref{eq:29} we can see
	\begin{equation*}
	\sigma + i\Omega = 
	\begin{pmatrix}
	2\Delta\hat{x}^2 & i  \\
	-i & 2\Delta\hat{p}^2 \\
	\end{pmatrix} \geq 0.
	\end{equation*}
	Taking the determinant gives
	\begin{equation*}
	4\Delta\hat{x}^{2}\Delta\hat{p}^2 - 1 \geq 0.
	\end{equation*}  
	This gives us Heisenberg's uncertainty principle ($\hbar = 1$) \cite{Heisenberg27} 
	\begin{equation*}
	\Delta\hat{x}^{2}\Delta\hat{p}^2 \geq \frac{1}{4} .
	\end{equation*}
	
	\section{Dynamics}
	\label{sec:dynamics}
	\subsection{The Heisenberg Picture}
	Till now, our discussion of the evolution of state has been described in terms of time independent operators acting upon a time varying state. This view of the world is described by the Schr{\"o}dinger picture in which all operators representing observables (Hamiltonian, position, time etc.) are constant in time. In contrast to this, however, the Heisenberg picture assumes the state of the system has no time dependency, rather it is acted upon by time dependent operators.
	
	Clearly, at $t=0$, both the Schr{\"o}dinger picture and the Heisenberg picture will coincide. Furthermore, the Heisenberg equation governs the dynamics of the system entirely analogous to the Schr{\"o}dinger equation \cite{Fujita}.
	
	
	\subsection{Closed System Dynamics}
	
	A closed system is one which experiences no influence from the external environment. The Heisenberg evolution equation for an operator $\hat{O}$ in a closed system is given by \cite{Fujita}
	\begin{equation*}
	\frac{d\hat{O}}{dt} = \frac{i}{\hbar}[\hat{H}, \hat{O}].
	\end{equation*}
	
	For any system, the vector of canonical operators $\mathbf{\hat{r}}$ evolves under the quadratic Hamiltonian, $\hat{H} = \frac{1}{2} H_{k l}\hat{r_k}\hat{r_l}$, according to \cite{Genoni16} 
	\begin{equation}\label{eq:31}
	\frac{d \mathbf{\hat{r}}}{dt} = \Omega H \mathbf{\hat{r}},
	\end{equation} which has the familiar solution
	\begin{equation}\label{eq:32}
	\mathbf{\hat{r}}(t) =e^{\Omega H t} \mathbf{\hat{r}}(0) = S\mathbf{\hat{r}}(0) ,
	\end{equation}
	where $S = e^{\Omega H t}$ is a real, symplectic matrix.
	
	\emph{Proof}
	\begin{flalign*}
	\frac{d\hat{r_j}}{dt} &= i[\frac{1}{2}H_{kl}\hat{r_k}\hat{r_l}, \hat{r_j}] = \frac{i}{2}H_{kl} [\hat{r_k}\hat{r_l}, \hat{r_j}]&\\
	&= \frac{i}{2} H_{kl}(\hat{r_k}[\hat{r_l}, \hat{r_j}] + [\hat{r_k}, \hat{r_j}]\hat{r_l})& \tag*{using $[\hat{AB}, \hat{C}] = \hat{A}[\hat{B},\hat{C}] + [\hat{A},\hat{C}]\hat{B}$}\\
	&= \frac{i}{2} H_{kl} (\hat{r_k}(i\Omega_{lj}) + (i\Omega_{kj})\hat{r_l})&\tag*{using Eq.\ref{eq:11}}\\
	&= -\frac{1}{2} H_{kl} (\hat{r_k}\Omega_{lj} + \Omega_{kj}\hat{r_l})&\\
	&= \frac{1}{2} H_{kl} (\hat{r_k}\Omega_{jl} + \Omega_{jk}\hat{r_l})& \tag*{using $\Omega_{jl} = - \Omega_{lj}$}\\
	&= \frac{1}{2} \Omega_{jk}H_{lk}\hat{r_k}  + \frac{1}{2} \Omega_{jk}H_{kl}\hat{r_l}&\\
	&= \Omega_{jl}H_{lk}\hat{r_k}& \tag*{using $H_{kl} = H_{lk}$.}&
	\end{flalign*}
	
	Hence, as required, we arrive at
	\begin{equation*}
	\frac{d \mathbf{\hat{r}}}{d t} = \Omega H \mathbf{\hat{r}}
	\tag*{\rule{2mm}{2mm}}
	\end{equation*}
	
	
	
	The symplectic transformations, $S$, introduced in Eqs.\ref{eq:23} and \ref{eq:24} are equivalent to unitary transformations, $\hat{U} = e^{-i\hat{H}t}$, in the Hilbert space. Therefore, the symplectic transformation, $S$, preserves the symplectic form, $\Omega$, as outlined in Section \ref{sec:symplectic} \cite{Wang07}. 
	
	  
	\subsection{Open System Dynamics}
	Next, we turn our attention to open system dynamics, which will be the topic of this thesis. As outlined in \cite{Genoni16}, the covariance matrix, $\sigma$, fully characterises the shape of the Gaussian and, therefore, captures all the dynamical properties of interest. We would like to apply these techniques to a quantum system that is under the influence of its external environment. We make the simplifying assumption that the system is Markovian; the evolution of the state is entirely determined by the current state and has no dependence on the state's history. 
	
	The evolution of $\sigma$ is described by
	\begin{equation} \label{eq:33}
	\frac{d \sigma}{d t} = (\Omega H) \sigma + \sigma(\Omega H)^T  ,
	\end{equation}
	with the solution
	\begin{equation} \label{eq:34}
	\sigma(t) = e^{\Omega H t} \sigma(0) (e^{\Omega H t})^T = S\sigma(0)S^T .
	\end{equation}
	
	\emph{Proof}
	\begin{flalign*}
	\frac{d\sigma_{jk}}{dt} &= \frac{d}{dt}(\Tr[\hat{\rho} \{\mathbf{\hat{r_j}}, \mathbf{\hat{r_k}}\}])& \tag*{using Eq.\ref{eq:19} with no linear terms}\\
	&= \Tr[\hat{\rho} \frac{d}{dt}(\mathbf{\hat{r_j}}\mathbf{\hat{r_k}} + \mathbf{\hat{r_k}}\mathbf{\hat{r_j}})]& \tag*{\text{as $\hat{\rho}$ has no time dependence}}\\
	&= \Tr[\hat{\rho} (\frac{d \mathbf{\hat{r_j}}}{dt} \mathbf{\hat{r_k}} + \mathbf{\hat{r_j}}\frac{d \mathbf{\hat{r_k}}}{d t} +\frac{d \mathbf{\hat{r_k}}}{d t} \mathbf{\hat{r_j}} + \mathbf{\hat{r_k}}\frac{d\mathbf{\hat{r_j}}}{dt})]&\\
	&= \Tr[\hat{\rho} (\Omega_{jm}H_{mn}\mathbf{\hat{r_n}\hat{r_k}} + \mathbf{\hat{r_j}}\Omega_{km}H_{mn}\mathbf{\hat{r_n}}  + \Omega_{km}H_{mn}\mathbf{\hat{r_n}\hat{r_j}} + \mathbf{\hat{r_k}}\Omega_{jm}H_{mn}\mathbf{\hat{r_n}} )]&\tag*{\text{using Eq.\ref{eq:31}}}\\
	&= \Omega_{jm}H_{mn}\sigma_{nk} + \Omega_{km}H_{mn}\sigma_{jn}&\\
	&= (\Omega_{jm}H_{mn})\sigma_{nk} + \sigma_{jn}(\Omega H)_{nk}^T .&
	\end{flalign*}
	
	Hence, as required, we arrive at
	\begin{equation*}
	\frac{d \sigma}{d t} = (\Omega H) \sigma + \sigma(\Omega H)^T  \tag*{\rule{2mm}{2mm}}
	\end{equation*}
	
	For the open system we can state a new equation of motion, the `diffusion equation' \cite{Genoni16}
	\begin{equation} \label{eq:35}
	\frac{d \sigma}{d t} = A\sigma +\sigma A^{T} + D,
	\end{equation} where $A$ and $D$ are the `drift' and `diffusion' matrices respectively. For a coupling matrix, $C$, between the system and the environment, $A$ and $D$ are defined as
	\begin{equation} \label{eq:36}
	A = \Omega H_S + \frac{\Omega C\Omega C^T}{2}\quad\text{and}\quad D = \Omega C \sigma_{E} C^{T} \Omega^T ,
	\end{equation} where $H_S$ refers to the Hamiltonian of the system described by Eq.\ref{eq:22} and $\sigma_E$ is the environment covariance matrix.
	
	Taking our simplifying assumption that the system is interacting with a Markovian bath and further assume that this bath is in thermal equilibrium, then $A$, the drift matrix, describes the dissipation in the thermal environment and $D$, the diffusion matrix, describes the thermal noise. We note that $A$ and $D$ are drift and diffusion matrices of a quantum system if and only if \cite{Serafini17}
	
	\begin{equation} \label{eq:37}
	D + iA \Omega^T - i\Omega A^T \geq 0.
	\end{equation}
		
	Within this thesis the emphasis will be on the steady state diffusion equation which can be found by setting Eq. \ref{eq:35} equal to zero
	\begin{equation} \label{eq:38}
	0 = A\sigma +\sigma A^{T} + D.
	\end{equation}
	
	\color{blue}QUESTION TO ASK In this state the system is relaxed and why are we looking specifically at steady state? \color{black}
	
	\section{Optimal Cooling Hamiltonian}
	
	Find a section to write motivation in, will basically be what's written in the introduction, but as a reminder?
	
	The entropy of the system is determined by the covariance matrix, $\sigma$, and cooling the system refers to draining entropy [REF]. Therefore, by removing the heat from a system, we move the system towards a pure state, which is what we need for quantum computing, for example [REF]. The entropy of the state can be found by the determinant of $\sigma$, therefore we want to find the smallest determinant possible in order to find the state with the lowest entropy, i.e. the coolest, and thus most pure, state. We obtain the minimum possible determinant by varying the Hamiltonian. This is all within an environment that is fixed, beyond our control. The optimal cooling Hamiltonian is therefore one which, when considered within the definition of $A$ as shown in \ref{eq:36}, provides the minimum determinant of $\sigma$. 
	
	The work within this thesis is limited to quadratic Hamiltonians, the form described by \ref{eq:22}. This does not limit us really as most noise, interactions etc involve quadratic Hamiltonians [REF] and this formalism applies to a range of platforms such as optomechanics [REF], quantum light [REF] and trapped ions [REF] to name only a few. 
	
	\subsection{The Steady State Covariance Matrix}
	\label{sec:sigmasolve}
	
	To find the optimal cooling Hamiltonian, we must first solve the steady state diffusion equation to find an expression for the steady state covariance matrix, $\sigma$. 
	
	We can solve for $\sigma$ using the following method
	\begin{align*}
	0 &= A\sigma +\sigma A^{T} + D&\\
	-D &= A\sigma +\sigma A^{T}&\\
	\Vect{(-D)} &= \Vect{(A\sigma +\sigma A^{T})}& \tag*{\text{where vec(X) refers to the vectorisation of a matrix X}}\\
	\Vect{(-D)} &= \Vect{(A\sigma)} + \Vect{(\sigma A^{T})}&\\
	\Vect{(-D)} &= (\mathbb{I} \otimes A)\Vect{(\sigma)} + (A \otimes \mathbb{I})\Vect{(\sigma)} & \tag*{\text{using $\Vect{(AB)}=(\mathbb{I} \otimes A)\Vect{(B)}=(B^{T} \otimes \mathbb{I})\Vect{(A)} $}}\\
	\Vect{(-D)} &= (\mathbb{I} \otimes A + A \otimes \mathbb{I}) \Vect{(\sigma)}.&\\
	(\mathbb{I} \otimes A + A \otimes \mathbb{I})^{-1}\Vect{(-D)} &= \Vect{(\sigma)}& \tag*{$\sigma$ can then be found by reshaping.}
	\end{align*}
	 
	\color{blue}Introduce that I'm going to consider the optimal cooling Hamiltonian in various cases, specifically one and two mode cases - ASK WHAT ON EARTH THESE ARE AND WHY WE TOOK THIS APPROACH \color{black}	
	
	\subsection{One Mode Case}
	\label{sec:onemode}
	\subsubsection{Definitions}
	\label{sec:onemodedefs}
	We will first consider the one mode \color{blue}[what does this mean] \color{black}case with $A$ and $D$ given as $2 \times 2$ matrices defined as follows
	\begin{align*}
	A &= A_0 + \Omega H \quad\text{where}\quad A_0 = -\frac{1}{2}\mathbb{I},&\\
	D &= \chi \mathbb{I}.&
	\end{align*}
	In the definition of $A$, $A_0$ describes the dissipation towards the environment and does not need to be of Hamiltonian form \color{blue}[what does this mean]\color{black}. $\Omega H$ describes the unitary transformation and we restrict $H$ to $H>0$.\color{blue}`we could actually also drop that restriction, there will be steady states all the same, like in parametric down conversion' from AS 08/08? \color{black} In the definition of $D$, $\chi$ represents the coupling constant and reflects the temperature. Within the one mode case we consider this one parameter for the environment. \color{red}We note that $\chi$ must be greater than 1 to be a valid coupling constant [REF]. \color{black} 
	
	To ensure that $A$ and $D$ are drift and diffusion matrices of a quantum system we must check the condition given in Eq. \ref{eq:37}. This can be written as
	
	\begin{align} 
	&D + iA \Omega^T - i\Omega A^T \geq 0& \nonumber\\
	&D + iA_0 (-\Omega) + i (\Omega H) (-\Omega) - i\Omega A_{0}^{T} - i\Omega (-H\Omega) \geq 0& \nonumber\\
	&D - iA_0 \Omega - i\Omega A_0 \geq 0& \label{eq:42}.
	\end{align}
	
	Using the definitions for $A$ and $D$, \ref{eq:42} becomes	
	\begin{equation*}
	\begin{pmatrix}
	\chi & 0  \\
	0 & \chi \\
	\end{pmatrix} + i\begin{pmatrix}
	\frac{1}{2} & 0  \\
	0 & \frac{1}{2} \\
	\end{pmatrix}\begin{pmatrix}
	0 & 1  \\
	-1 & 0 \\
	\end{pmatrix} + i\begin{pmatrix}
	0 & 1  \\
	-1 & 0 \\
	\end{pmatrix}\begin{pmatrix}
	\frac{1}{2} & 0  \\
	0 & \frac{1}{2} \\
	\end{pmatrix},
	\end{equation*}
	resulting in the matrix to evaluate as
	\begin{equation*}
	\begin{pmatrix}
	\chi & i  \\
	-i & \chi \\
	\end{pmatrix}.
	\end{equation*}
	We can check eigenvalues to see whether the matrix is positive-definite, as required for the condition to hold. The eigenvalues are given by
	\begin{align*}
	&(\chi- \lambda)(\chi- \lambda) - 1 = 0&\\
	&\lambda = \chi \pm 1 \geq 0&\tag*{as $\chi \geq 1$}
	\end{align*}
	Therefore, $A$ and $D$ represent the drift ad diffusion matrices of a quantum system.
	
	
	Having solved for sigma using the method outlined in Section \ref{sec:sigmasolve}, we can find the minimum determinant of $\sigma$ by varying the Hamiltonian, $H$, where the environment parameter, $\chi$, is fixed. We vary the Hamiltonian under the constraint that it must be positive-definite \color{red}Write whatever is in line with the decision with this earlier\color{black}. 
	
	The Cholesky decomposition of a positive-definite matrix such as the Hamiltonian, $H$, is of the form
	\begin{equation*}
	H = LL^{T},
	\end{equation*}
	where $L$ is referred to as the lower triangular matrix. 
	The lower triangular matrix has the form \begin{equation}\label{eq:39}
	L = \begin{pmatrix}
	a & 0  \\
	b & c\\
	\end{pmatrix},
	\end{equation}
	resulting in a Hamiltonian of the form
	\begin{equation} \label{eq:41}
	H = \begin{pmatrix}
	a^{2} & ab  \\
	ab & b^{2}+c^{2}\\
	\end{pmatrix}.
	\end{equation}
	This allows us to find the minimum determinant of the covariance matrix, $\sigma$, by varying the real parameters $a$, $b$ and $c$ of the Hamiltonian.
	
	\color{red}\subsection{Method}\color{black}
	What was the method that we used to get the below results - explain
	
	\subsubsection{Minimum Determinant}
	
	We found that the minimum determinant of the covariance matrix, $\sigma$, is given by $\chi^{2}$, with $\det \sigma \geq \chi^{2}$. 
	
	\color{red}I DON'T LIKE THIS BIT, LOOKS SUPER MESSY AND IS RUBBISH \color{black}
	
	Using the definitions given for $A$ and $D$, the steady state diffusion equation (Eq. \ref{eq:38}) can be written as
	\begin{align}	
	0 &= (-\frac{1}{2}\mathbb{I} + \Omega H)\sigma +\sigma (-\frac{1}{2}\mathbb{I} + \Omega H)^{T} + D&\nonumber\\
	0 &= -\sigma + (\Omega H)\sigma +\sigma (\Omega H)^{T} + D& \nonumber\\
	0 &= -\sigma + \Omega H\sigma +\sigma H\Omega^{T} + D \nonumber&\\
	0 &= -\sigma + \Omega H\sigma -\sigma H\Omega + \chi \mathbb{I} & \tag*{ as $\sigma$ is symmetric.}
	\end{align}
	
	If we were to take the trivial case of $H=0$ \color{red}There is no Hamiltonian?\color{black}, the above gives $-\sigma = -D$, thus $\sigma = \chi \mathbb{I}$. The determinant of the covariance matrix, $\sigma$, would then be given as $\det\sigma = \chi^{2}$. \color{red}Why are we looking at $H=0$ if we have said multiple times, $H>0$??\color{black}
	
	\color{blue}\emph{Proof}
	
	Choice of $A$ and $D$ are independent - huh? 
	
	Within the steady state diffusion equation Eq. \ref{eq:38}, the covariance matrix, $\sigma$, represents a physical state with $A$ and $D$ representing the drift and diffusion matrices of a quantum system. We can divide by $\chi$ to obtain
	\begin{equation*}
	0 = A \frac{\sigma}{\chi} + \frac{\sigma}{\chi} A + \mathbb{I}.
	\end{equation*}
	
	Here $\frac{\sigma}{\chi}$ is a steady state solution to the linear equation, and represents physical state. We must therefore have that $\det\frac{\sigma}{\chi} \geq \mathbb{I}$ [REF], resulting in $\det \sigma \geq \chi^2$. 
	
	The minimum of this would be the case where $H=0$, where $\det\frac{\sigma}{\chi} = \mathbb{I}$, thus the minimum determinant of the covariance matrix, $\sigma$ must be $\det \sigma = \chi^2$.
	\begin{equation*}
	\tag*{\rule{2mm}{2mm}}
	\end{equation*}\color{black}
	
	\subsubsection{Optimal Hamiltonian}
	Within the setup, we are unable to change the environment and so we are looking for the form of the Hamiltonian \color{blue}(this is what we can change, whether it is actually a feasible H is another question we'll think about later...check with AS).\color{black} that gives us this minimum determinant for $\sigma$.
	
	We found that the optimal cooling Hamiltonian is of the form 
	
	\begin{equation*}
	H =  \begin{pmatrix}
	a^2 & 0  \\
	0 & a^2\\
	\end{pmatrix}.
	\end{equation*}

	
	Then in Eq. \ref{eq:37} we have 
	\begin{align*}
	0 &= -\sigma + a^2 (\Omega \sigma + \sigma \Omega^{T}) + \chi \mathbb{I}&\\
	\text{Suppose $\sigma$ has the form $\alpha \mathbb{I}$}&\\
	0 &= -\alpha \mathbb{I} + \alpha a^2 (\Omega + \Omega^T) + \chi \mathbb{I}&\\
	-\alpha \mathbb{I} & = -\chi \mathbb{I} & \tag*{as $\Omega$ is anti-symmetric}&\\
	\chi &= \alpha&
	\end{align*}
	
	This gives 
	
	\begin{equation*}
	\sigma = \begin{pmatrix}
	\chi & 0  \\
	0 & \chi\\
	\end{pmatrix}
	\end{equation*} and $\det \sigma = \chi^2$ 

	\subsection{Squeezed state}
	What if $D$ is no longer a scaled identity, what if instead it is a squeezed state (A squeezed state is ...).
	
	\begin{align*}
	A &= A_0 + \Omega H \quad\text{where}\quad A_0 = -\frac{1}{2}\mathbb{I}&\\
	D &= \begin{pmatrix}
	z & 0  \\
	0 & \frac{1}{z}\\
	\end{pmatrix},&
	\end{align*} where $z$ is varied with the constraint that $z>1$. 
	
	This is implemented in `solver.py'.
	
	Check that \ref{eq:42} condition still holds:
	\begin{align*}
	\begin{pmatrix}
	z & 0  \\
	0 & \frac{1}{z}\\
	\end{pmatrix} + i\begin{pmatrix}
	\frac{1}{2} & 0  \\
	0 & \frac{1}{2} \\
	\end{pmatrix}\begin{pmatrix}
	0 & 1  \\
	-1 & 0 \\
	\end{pmatrix} + i\begin{pmatrix}
	0 & 1  \\
	-1 & 0 \\
	\end{pmatrix}\begin{pmatrix}
	\frac{1}{2} & 0  \\
	0 & \frac{1}{2} \\
	\end{pmatrix}&\\
	&\begin{pmatrix}
	z & i  \\
	-i & \frac{1}{z} \\
	\end{pmatrix}&
	\end{align*}
	Check eigenvalues
	\begin{align*}
	&(z- \lambda)(\frac{1}{z} - \lambda) - 1 = 0&\\
	&\lambda(\lambda - (z+ \frac{1}{z})) = 0&\\
	&\lambda = 0\quad\text{or}\quad\lambda= z+ \frac{1}{z} \geq 0&\tag*{as $z \geq 1$}
	\end{align*} so condition holds? 
	
	\subsubsection{Minimum Determinant}
	Determinant minimum is still $\chi^{2}$.
	
	\color{red}Do I need to prove this? \emph{Proof}\color{black}

	\color{red}\subsubsection{Method}\color{black}

	
	\subsection{Two mode case}
	\subsubsection{Definitions}
	Now considering a two mode case (...) with $A$ and $D$ given as 4x4 matrices
	\begin{align*}
	A &= A_0 + \Omega H \quad\text{where}\quad A_0 = -\frac{1}{2}\mathbb{I}_4\quad\text{and}\quad \Omega = \begin{pmatrix}
	0 & 1 & 0 & 0 \\
	-1 & 0 & 0 & 0\\
	0 & 0 & 0 & 1\\
	0 & 0 & -1 & 0\\
	\end{pmatrix} &\\
	D &= \chi \mathbb{I}&
	\end{align*} (Same as what was considered in Section \ref{sec:onemode} for $A$ and $D$).
	
	Using the Cholesky decomposition outlined in Section \ref{sec:onemode} we can construct a positive definite Hamiltonian, $H$, with 10 real parameters and solve for $\sigma$. We can then vary these parameters to find the minimum determinant of the one-mode $\sigma$ (top left 2x2 matrix in the 4x4 sigma). 
	
	Can check if the required condition for $A$ and $D$ holds:

	\begin{align*}
	&...\begin{pmatrix}
	\chi & 0 & 0 & 0\\
	0 & \chi & 0 & 0\\
	0 & 0 & \chi & 0 \\
	0 & 0 & 0 & \chi\\
	\end{pmatrix} + \frac{i}{2}\begin{pmatrix}
	1 & 0 & 0 & 0\\
	0 & 1 & 0 & 0\\
	0 & 0 & 1 & 0 \\
	0 & 0 & 0 & 1\\
	\end{pmatrix}\begin{pmatrix}
	0 & 1 & 0 & 0 \\
	-1 & 0 & 0 & 0\\
	0 & 0 & 0 & 1\\
	0 & 0 & -1 & 0\\
	\end{pmatrix}&\\
	&+ \frac{i}{2}\begin{pmatrix}
	0 & 1 & 0 & 0 \\
	-1 & 0 & 0 & 0\\
	0 & 0 & 0 & 1\\
	0 & 0 & -1 & 0\\
	\end{pmatrix}\begin{pmatrix}
	1 & 0 & 0 & 0\\
	0 & 1 & 0 & 0\\
	0 & 0 & 1 & 0 \\
	0 & 0 & 0 & 1\\
	\end{pmatrix}&\\
	&...\begin{pmatrix}
	\chi & 0 & 0 & 0\\
	0 & \chi & 0 & 0\\
	0 & 0 & \chi & 0 \\
	0 & 0 & 0 & \chi\\
	\end{pmatrix} + \frac{i}{2}\begin{pmatrix}
	0 & 1 & 0 & 0 \\
	-1 & 0 & 0 & 0\\
	0 & 0 & 0 & 1\\
	0 & 0 & -1 & 0\\
	\end{pmatrix}  + \frac{i}{2}\begin{pmatrix}
	0 & 1 & 0 & 0 \\
	-1 & 0 & 0 & 0\\
	0 & 0 & 0 & 1\\
	0 & 0 & -1 & 0\\
	\end{pmatrix}&\\
	&...\begin{pmatrix}
	\chi & i & 0 & 0 \\
	-i & \chi & 0 & 0\\
	0 & 0 & \chi & i\\
	0 & 0 & -i & \chi\\
	\end{pmatrix}&
	\end{align*}	

	Check the eigenvalues of top left block and bottom right?  
	\begin{align*}
	&(\chi- \lambda)(\chi- \lambda) - 1&\\
	&\chi^2 - 1 - 2\chi \lambda + \lambda^2&\\
	&\lambda = \chi \pm 1 \geq 0& \tag*{as $\chi \geq 1$}
	\end{align*} so condition holds?
	
	
	\subsection{Minimum Determinant}
	Determinant minimum is $\chi^{2}$ (determinant $\geq \chi^{2}$).
		
	\subsection{Optimal Hamiltonian}
	Optimal Hamiltonian is $A = \alpha \mathbb{I}$, $B, C = 0$, $D$ is arbitrary in
	\begin{equation*}
	H =
	\left[
	\begin{array}{c|c}
	A & B \\
	\hline
	C & D
	\end{array}
	\right]
	\end{equation*}
		
	\begin{equation}\label{eq:43}
	H =
	\left[
	\begin{array}{c|c}
	\alpha \mathbb{I} & 0 \\
	\hline
	0 & D
	\end{array}
	\right]
	\end{equation}
	
	Ideal case $H=0$, however need to main that the Hamiltonian is positive-definite ($H>0$) so take the identity matrix as an example of a positive-definite matrix that will give the minimum determinant for a given value of $\chi$. 

	If the Hamiltonian is of the form in Eq. \ref{eq:40} then 
	
	\begin{equation*}
	0 = A\sigma +\sigma A^{T} + D
	\end{equation*} 
	
	\begin{align*}
	A &= -\frac{1}{2}\mathbb{I} + \Omega H&\\
	 &= -\frac{1}{2} \begin{pmatrix}
	 1 & 0  \\
	 0 & 1\\
	 \end{pmatrix} +  \begin{pmatrix}
	 \Omega & 0  \\
	 0 & \Omega\\
	 \end{pmatrix}
	 \begin{pmatrix}
	 H_{11} & H_{12}  \\
	 H_{21} & H_{22}\\
	 \end{pmatrix}&\\
	  &=  -\frac{1}{2} \begin{pmatrix}
	  1 & 0  \\
	  0 & 1\\
	  \end{pmatrix} +  \begin{pmatrix}
	  \Omega H_{11} & 0  \\
	  0 & \Omega H_{22}\\
	  \end{pmatrix}&\tag*{as $H_{12} = H_{21} = 0$}&\\
	  &= \begin{pmatrix}
	  \Omega H_{11} -\frac{1}{2}\mathbb{I} & 0  \\
	  0 & \Omega H_{22}-\frac{1}{2}\mathbb{I}\\
	  \end{pmatrix}&\\
	  &= \begin{pmatrix}
	  A_{11} & 0  \\
	  0 & A_{22}\\
	  \end{pmatrix}&
	\end{align*}
	
	\begin{equation*}
	 -\chi \begin{pmatrix}
	\mathbb{I} & 0  \\
	0 & \mathbb{I}\\
	\end{pmatrix} = \begin{pmatrix}
	A_{11} & 0  \\
	0 & A_{22}\\
	\end{pmatrix}\begin{pmatrix}
	\sigma_{11} & \sigma_{12}  \\
	\sigma_{21} & \sigma_{22}\\
	\end{pmatrix} + \begin{pmatrix}
	\sigma_{11} & \sigma_{12}  \\
	\sigma_{21} & \sigma_{22}\\
	\end{pmatrix}\begin{pmatrix}
	A_{11}^T & 0  \\
	0 & A_{22}^T\\
	\end{pmatrix}
	\end{equation*}
	
	... try $\sigma$ is identity matrix and it works so solution is $\sigma$ is a scaled identity.... 
	
	\color{red}\subsection{Method}\color{black}
	
 	\iffalse
	\section{Something about rotations?}
	 
	Eq. \ref{eq:37} can be written as
	\begin{align*} 
	0 = -\sigma + \Omega H\sigma -\sigma H\Omega + \chi \mathbb{I} 
	\end{align*} as $\sigma$ is symmetric. 
	
	The symplectic transformation $R\Omega R^{T} = \Omega$ where $RR^{T}=1$

	\begin{align*} 
	0 &= -R\sigma R^{T} + R\Omega H\sigma R^{T} - R\sigma H\Omega R^{T} + \chi \mathbb{I}&\\
	0 &= -R\sigma R^{T} + \Omega R H R^T R \sigma R^{T} - R\sigma R^{T} RH R^T\Omega + \chi \mathbb{I}& 
	\end{align*}	using $R\Omega = \Omega R$. Then, defining $\sigma' = R\sigma R^T$
	
	\begin{equation*} 
	0 = -\sigma' + \Omega R H R^T \sigma' - \sigma RH R^T\Omega + \chi \mathbb{I}
	\end{equation*}
	
	Symplectic transformations do not affect the local determinant so R must be local and passive, there's only one choice (??).
	
	Let $RHR^{-1} = H'$ and the doing an SVD with $R$ defined as
	\begin{equation*}
	R = \begin{pmatrix}
	cos(\theta) & sin(\theta) & 0 & 0\\
	-sin(\theta) & cos(\theta) & 0 & 0\\
	0 & 0 & cos(\phi) & sin(\phi) \\
	0 & 0 & -sin(\phi) & cos(\phi)\\
	\end{pmatrix}
	\end{equation*}

	\begin{equation*}
	\begin{pmatrix}
	cos(\theta) & sin(\theta) & 0 & 0\\
	-sin(\theta) & cos(\theta) & 0 & 0\\
	0 & 0 & cos(\phi) & sin(\phi) \\
	0 & 0 & -sin(\phi) & cos(\phi)\\
	\end{pmatrix}	\begin{pmatrix}
	H_{11} & H_{12}\\
	\\
	H^{T}_{12} & H_{22} \\
	\\
	\end{pmatrix}	\begin{pmatrix}
	cos(\theta) & -sin(\theta) & 0 & 0\\
	sin(\theta) & cos(\theta) & 0 & 0\\
	0 & 0 & cos(\phi) & -sin(\phi) \\
	0 & 0 & sin(\phi) & cos(\phi)\\
	\end{pmatrix}
	\end{equation*}	
	
	This will give us a diagonal matrix $H$ where the diagonal values are the eigenvalues. 
	\fi
	
	\section{Cooling in Optomechanics}
	\subsection{Definitions}
	\label{sec:optodefs}	
	Drift matrix defined as $A_0 +\Omega H$ with
	\begin{equation*}
	A_0 = -\frac{1}{2} \begin{pmatrix}
	\kappa_1 & 0\\
	0 & \kappa_{2}\\
	\end{pmatrix} \otimes \mathbb{I}_2
	\end{equation*} where $\kappa_1$ is phonon leakage rate, $\kappa_{2}$ is photon leakage rate with $\kappa_{2} \geq \kappa_{1} \geq 1$.
	
	Diffusion matrix defined as
	\begin{equation*}
	D = \begin{pmatrix}
	\chi_1 & 0\\
	0 & \chi_2\\
	\end{pmatrix} \otimes \mathbb{I}_2
	\end{equation*} where $\chi_1 \geq \chi_2 \geq 1$ give the number of phonons, photons populating the bath at room temperature.
	
	The relationship between $\chi$ and $\kappa$ can be found using Eq.\ref{eq:42}.

	
	This becomes using the definition of $A$ and $D$ outlined in Section \ref{sec:optodefs}
	\begin{align*}
	&...\begin{pmatrix}
	\chi_1 & 0 & 0 & 0\\
	0 & \chi_1 & 0 & 0\\
	0 & 0 & \chi_2 & 0 \\
	0 & 0 & 0 & \chi_2\\
	\end{pmatrix} + \frac{i}{2}\begin{pmatrix}
	\kappa_1 & 0 & 0 & 0\\
	0 & \kappa_1 & 0 & 0\\
	0 & 0 & \kappa_2 & 0 \\
	0 & 0 & 0 & \kappa_2\\
	\end{pmatrix}\begin{pmatrix}
	0 & 1 & 0 & 0 \\
	-1 & 0 & 0 & 0\\
	0 & 0 & 0 & 1\\
	0 & 0 & -1 & 0\\
	\end{pmatrix}&\\
	&+ \frac{i}{2}\begin{pmatrix}
	0 & 1 & 0 & 0 \\
	-1 & 0 & 0 & 0\\
	0 & 0 & 0 & 1\\
	0 & 0 & -1 & 0\\
	\end{pmatrix}\begin{pmatrix}
	\kappa_1 & 0 & 0 & 0\\
	0 & \kappa_1 & 0 & 0\\
	0 & 0 & \kappa_2 & 0 \\
	0 & 0 & 0 & \kappa_2\\
	\end{pmatrix}&\\
	&...\begin{pmatrix}
	\chi_1 & 0 & 0 & 0\\
	0 & \chi_1 & 0 & 0\\
	0 & 0 & \chi_2 & 0 \\
	0 & 0 & 0 & \chi_2\\
	\end{pmatrix} + \frac{i}{2}\begin{pmatrix}
	0 & \kappa_1 & 0 & 0 \\
	-\kappa_1 & 0 & 0 & 0\\
	0 & 0 & 0 & \kappa_2\\
	0 & 0 & -\kappa_2 & 0\\
	\end{pmatrix}  + \frac{i}{2}\begin{pmatrix}
	0 & \kappa_1 & 0 & 0 \\
	-\kappa_1 & 0 & 0 & 0\\
	0 & 0 & 0 & \kappa_2\\
	0 & 0 & -\kappa_2 & 0\\
	\end{pmatrix}&\\
	&...\begin{pmatrix}
	\chi_1 & \kappa_1 & 0 & 0 \\
	-\kappa_1 & \chi_1 & 0 & 0\\
	0 & 0 & \chi_2 & \kappa_2\\
	0 & 0 & -\kappa_2 & \chi_2\\
	\end{pmatrix}&	
	\end{align*}
	
	Consider each block individually as they are completely distinct.
	
	Determinant top left gives $\chi_{1}^2 - \kappa_{1}^2 \geq 0$, $\chi_1 \geq \kappa_1$ and other gives $\chi_{2}^2 - \kappa_{2}^2 \geq 0$, $\chi_2 \geq \kappa_2$. This shows $\chi \geq \kappa$. \color{red} What's the relation between $\chi_1, \chi_2, \kappa_1, \kappa_2$
	
	And we can also see that the required condition for $A$ and $D$ holds...Check the eigenvalues: 
	\begin{align*}
	&(\chi_1- \lambda)(\chi_1- \lambda) - \kappa_{1}^2&\\
	&\chi_{1}^2 - \kappa_{1}^2 - 2\chi_{1}\lambda + \lambda^2&\\
	&\lambda = \chi_1 \pm \kappa_1 \geq 0& \tag*{as $\chi_1 \geq \kappa_1$}
	\end{align*} so condition holds? \color{black}

	\subsection{Minimum Determinant}
	The minimum determinant is given by $\Big(\dfrac{\chi_1 + \chi_2}{\kappa_1 + \kappa_2}\Big)^2$.  
	
	Because we know that $\chi \geq \kappa$ this determinant will always be $\geq 1$, as required. 
	
	\subsection{Optimal Hamiltonian}
	
	In the limit that $N \rightarrow \infty$, the optimal Hamiltonian is given by
	
	\begin{equation*}
	H = N \begin{pmatrix}
	1 & 1  \\
	1 & 1\\
	\end{pmatrix} \otimes \mathbb{I}_2.
	\end{equation*}
	
	The solution to $\sigma$ for finite $N$ is given by 
	
	\begin{equation*}
	\sigma = \begin{pmatrix}
	a & 0 & 0 & c\\
	0 & a & -c & 0\\
	0 & -c & b & 0 \\
	c & 0 & 0 & b\\
	\end{pmatrix}
	\end{equation*}	
	where, for convenience, we define $\alpha := \Big(\dfrac{\chi_1 + \chi_2}{\kappa_1 + \kappa_2}\Big)$ and $\beta := 1 + \frac{\kappa_1 \kappa_2}{4N^2}$ and then define
	
	
	\begin{align*}
	&a := \frac{\alpha + \frac{\chi_1 \kappa_2}{4N^2}}{\beta}&\\
	&b := \frac{\alpha + \frac{\chi_2 \kappa_1}{4N^2}}{\beta}&\\
	&c := \frac{\chi_1 \kappa_2 - \kappa_1 \chi_2}{2N\beta (\kappa_1+\kappa_2)}&		
	\end{align*}


It is straightforward to show that this expression for $\sigma$ satisfies the steady state diffusion equation:

	\begin{align*}
	&A\sigma +\sigma A^{T}&\\
	&= \begin{pmatrix}
	-\frac{\kappa_1}{2} & N & 0 & N\\
	-N & -\frac{\kappa_1}{2} & -N & 0 \\
	0 & N & -\frac{\kappa_2}{2} & N\\
	-N & 0 & -N & -\frac{\kappa_2}{2}\\
	\end{pmatrix}\begin{pmatrix}
	a & 0 & 0 & c\\
	0 & a & -c & 0\\
	0 & -c & b & 0 \\
	c & 0 & 0 & b\\
	\end{pmatrix} + \begin{pmatrix}
	a & 0 & 0 & c\\
	0 & a & -c & 0\\
	0 & -c & b & 0 \\
	c & 0 & 0 & b\\
	\end{pmatrix}\begin{pmatrix}
	-\frac{\kappa_1}{2} & -N & 0 & -N\\
	N & -\frac{\kappa_1}{2} & N & 0 \\
	0 & -N & -\frac{\kappa_2}{2} & -N\\
	N & 0 & N & -\frac{\kappa_2}{2}\\
	\end{pmatrix}&\\
	&= \begin{pmatrix}
	-\kappa_1 a - 2Nc & 0 & 0 & \frac{1}{2}(\kappa_1 + \kappa_2)c + N(b-a)\\
	0 & -\kappa_1 a - 2Nc & -\frac{1}{2}(\kappa_1 + \kappa_2)c - N(b-a) & 0 \\
	0 & -\frac{1}{2}(\kappa_1 + \kappa_2)c - N(b-a) & -\kappa_2 b + 2Nc & 0\\
	\frac{1}{2}(\kappa_1 + \kappa_2)c + N(b-a) & 0 & 0 & -\kappa_2 b + 2Nc\\
	\end{pmatrix}&\\
	&= \begin{pmatrix}
	-\chi_1 & 0 & 0 & 0\\
	0 & -\chi_1 & 0 & 0 \\
	0 & 0 & -\chi_2 & 0\\
	0 & 0 & 0 & -\chi_2\\
	\end{pmatrix}&\\
	&= -D&
	\end{align*}
	
	To verify this, first we show that the anti-diagonal terms are zero. Evaluating $\frac{1}{2}(\kappa_1 + \kappa_2)c$ and $N(b-a)$ separately
	\begin{align*}
	&\frac{1}{2}(\kappa_1 + \kappa_2) c = \frac{1}{2}(\kappa_1 + \kappa_2) \cdot \frac{\chi_1 \kappa_2 - \kappa_1 \chi_2}{2N\beta (\kappa_1+\kappa_2)} = \frac{1}{4N\beta}(\chi_1 \kappa_2 - \kappa_1 \chi_2)&\\
	&N(b-a)= N \Big(\frac{\alpha + \frac{\chi_2 \kappa_1}{4N^2}}{\beta} -\frac{\alpha + \frac{\chi_1 \kappa_2}{4N^2}}{\beta}\Big) = \frac{1}{4N\beta}(\chi_2 \kappa_1 - \chi_1 \kappa_2)&\\
	&\text{gives}\quad\frac{1}{2}(\kappa_1 + \kappa_2)c + N(b-a) = 0\text{ as required.}&
	\end{align*}
	
	Then, evaluating the diagonal terms $-\kappa_1 a - 2Nc$ and $-\kappa_2 b + 2Nc$
	
	\begin{align*}
	-\kappa_1 a - 2Nc &= \frac{-\kappa_1}{\beta} \Big(\alpha + \frac{\chi_1 \kappa_2}{4N^2} \Big) - \frac{\chi_1 \kappa_2 - \kappa_1 \chi_2}{\beta (\kappa_1+\kappa_2)}&\\
	&= \frac{-\kappa_1 (\kappa_1+\kappa_2) \Big(\alpha + \frac{\chi_1 \kappa_2}{4N^2} \Big) - \chi_1 \kappa_2 - \kappa_1 \chi_2}{\beta (\kappa_1+\kappa_2)}&\\
	&= \frac{-\chi_1(\kappa_1+\kappa_2)- (\kappa_1+\kappa_2)\chi_1 \frac{\kappa_1 \kappa_2}{4N^2}}{\beta (\kappa_1+\kappa_2)}&\\
	&= -\chi_1 \frac{\beta}{\beta}&\\
	&= -\chi_1&
	\end{align*}
	
	\begin{align*}
	-\kappa_2 b + 2Nc &= \frac{-\kappa_2}{\beta} \Big(\alpha + \frac{\chi_2 \kappa_1}{4N^2} \Big) + \frac{\chi_1 \kappa_2 - \kappa_1 \chi_2}{\beta (\kappa_1+\kappa_2)}&\\
	&= \frac{-\kappa_2 (\kappa_1+\kappa_2) \Big(\alpha + \frac{\chi_2 \kappa_1}{4N^2} \Big) + \chi_1 \kappa_2 - \kappa_1 \chi_2}{\beta (\kappa_1+\kappa_2)}&\\
	&= \frac{-\chi_2(\kappa_1+\kappa_2)- (\kappa_1+\kappa_2)\chi_2 \frac{\kappa_1 \kappa_2}{4N^2}}{\beta (\kappa_1+\kappa_2)}&\\
	&= -\chi_2 \frac{\beta}{\beta}&\\
	&= - \chi_2&
	\end{align*}
	

	In limit that $N \rightarrow \infty$ we first note that $\beta \rightarrow 1$. Then, inspecting terms $a$ and $b$ we see that $a \rightarrow \alpha$, $b \rightarrow \alpha$ and the determinant therefore converges on the minimum determinant $\alpha^2$.
	
	
	\iffalse
	Solution
	\begin{equation*}
	\sigma = \begin{pmatrix}
	\alpha & 0 & 0 & \epsilon\\
	0 & \alpha & -\epsilon & 0\\
	0 & -\epsilon & \alpha & 0 \\
	\epsilon & 0 & 0 & \alpha\\
	\end{pmatrix}
	\end{equation*}	
	where $\alpha = \Big(\dfrac{\chi_1 + \chi_2}{\kappa_1 + \kappa_2}\Big)$ and $\epsilon = \frac{\big(\frac{\chi_1 + \chi_2}{\kappa_1 + \kappa_2}\big) \frac{\kappa_1}{2} - \frac{\chi_1}{2}}{N} = \frac{\alpha \kappa_1 - \chi_1}{2N}$.
	
	\begin{align*}
	&A\sigma +\sigma A^{T}&\\
	&= \begin{pmatrix}
	-\frac{\kappa_1}{2} & N & 0 & N\\
	-N & -\frac{\kappa_1}{2} & -N & 0 \\
	0 & N & -\frac{\kappa_2}{2} & N\\
	-N & 0 & -N & -\frac{\kappa_2}{2}\\
	\end{pmatrix}		\begin{pmatrix}
	\alpha & 0 & 0 & \epsilon\\
	0 & \alpha & -\epsilon & 0\\
	0 & -\epsilon & \alpha & 0 \\
	\epsilon & 0 & 0 & \alpha\\
	\end{pmatrix} + \begin{pmatrix}
	\alpha & 0 & 0 & \epsilon\\
	0 & \alpha & -\epsilon & 0\\
	0 & -\epsilon & \alpha & 0 \\
	\epsilon & 0 & 0 & \alpha\\
	\end{pmatrix}	\begin{pmatrix}
	-\frac{\kappa_1}{2} & -N & 0 & -N\\
	N & -\frac{\kappa_1}{2} & N & 0 \\
	0 & -N & -\frac{\kappa_2}{2} & -N\\
	N & 0 & N & -\frac{\kappa_2}{2}\\
	\end{pmatrix}&\\
	&= \begin{pmatrix}
	2N \epsilon - \kappa_1 \alpha & 0 & 0 & -\frac{1}{2} \epsilon (\kappa_1 + \kappa_2)\\
	0 & 2N \epsilon - \kappa_1 \alpha & -\frac{1}{2} \epsilon (\kappa_1 + \kappa_2) & 0 \\
	0 & \frac{1}{2} \epsilon (\kappa_1 + \kappa_2) & -2N \epsilon - \kappa_2 \alpha & 0\\
	\frac{1}{2} \epsilon (\kappa_1 + \kappa_2) & 0 & 0 & -2N \epsilon - \kappa_2 \alpha\\
	\end{pmatrix}&\\
	&= \begin{pmatrix}
	-\chi_1 & 0 & 0 & \mathcal{O}(N^{-1})\\
	0 & -\chi_1 & \mathcal{O}(N^{-1}) & 0 \\
	0 & \mathcal{O}(N^{-1}) & -\chi_2 & 0\\
	\mathcal{O}(N^{-1}) & 0 & 0 & -\chi_2\\
	\end{pmatrix}&\\
	&= -D& \tag*{so $\sigma$ is the steady state solution in the asymptotic limit.}
	\end{align*}
	\fi
	
	\color{red}\subsection{Method}
	
	
		
	\subsection{Beam Splitter Hamiltonian}

	
	Then consider H of the form (Beamsplitter Hamiltonian):
	\begin{equation*}
	H = g\begin{pmatrix}
	0 & 0 & 0 & -1\\
	0 & 0 & 1 & 0\\
	0 & 1 & 0 & 0 \\
	-1 & 0 & 0 & 0 \\
	\end{pmatrix}
	\end{equation*} where $g$ is the Hamiltonian strength (interaction strength?). Is this optimal form? NO.
	
	What value of $g$ gives you the minimum determinant of $\sigma$. Is there a formula that will give us the best g, for your given chi and kappa values. 
	

	\section{Conclusion}
	\label{sec:conc}
	\color{black}
	
	
	\bibliography{bibl_file}
	\bibliographystyle{ieeetr}
	
	
\end{document} 