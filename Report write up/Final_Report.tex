\documentclass[11pt,a4paper]{article}
\usepackage{bm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{physics}
\usepackage[margin=1.25in]{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{color}
\usepackage{amsmath}
\usepackage[title]{appendix}
\usepackage{mathtools}
\numberwithin{equation}{section}
\linespread{1.3}
\DeclareMathOperator{\Vect}{vec}

\let\OLDthebibliography\thebibliography
\renewcommand\thebibliography[1]{
	\OLDthebibliography{#1}
	\setlength{\parskip}{0pt}
	\setlength{\itemsep}{0pt plus 0.3ex}
}

\begin{document}
	\pagenumbering{arabic}
	\begin{titlepage}
		\begin{center}
			\vspace*{1cm}
			
			\textbf{\LARGE{Hamiltonian Control of Open Gaussian Systems}}
			
			\vspace{0.5cm}
			
			\large{Rianna Kelly}
			
			\vfill
			
			A thesis submitted in partial fulfilment\\
			of the requirements for the degree of\\
			MSc Quantum Technologies
			
			\vspace{1cm}
			
			Department of Physics and Astronomy\\
			University College London\\
			August 2017
			
			\vspace{0.25\textheight}
			
			
		\end{center}
	\end{titlepage}

	\begin{abstract}
		
	We examine the steady state diffusion equation with the aim of deriving the Hamiltonian that achieves the lowest entropy and thus maximal cooling for a given configuration. We consider both the one and two mode cases with the latter case playing an important role in the field of optomechanics and non-classical properties such as entanglement. We present an analytical proof of the lower bound on the determinant of the covariance matrix, $\sigma$, in the one mode case and verify our findings numerically. In the two mode case, we propose a lower bound on the $2 \times 2$ sub determinant and verify the hypothesis numerically. DON'T I WANT TO TALK ABOUT THE HAMILTONIANS, I KNOW THE AIM WAS TO LOWER THE DET BUT DIDN'T I DO THAT BY FINDING THE OPTIMAL HAMILTONIAN?
	
	\end{abstract}
		
	\tableofcontents
	\listoffigures

	\newpage
		
	\section{Introduction}
	
	
	In the conclusions and intro, I recommend you to stress the main results: the analytical proof that H=0 is the best one can do for one mode, and the numerical finding about the minimum local determinant for two modes.
	Lorem ipsum dolor sit amet, consectetur adipiscing elit. Quisque lacus sem, ornare in commodo venenatis, pellentesque eget eros. Praesent at laoreet leo. Aliquam vitae elit eu tortor malesuada luctus at non lectus. Ut dapibus quis est nec facilisis. Phasellus et vehicula tellus. Suspendisse ut fringilla tortor. Aenean convallis hendrerit maximus. Ut aliquet purus in eros tincidunt, aliquet semper quam gravida. Donec nec augue quis eros malesuada scelerisque et non neque. Curabitur et orci consequat, vehicula neque vitae, facilisis mauris. Aliquam nec interdum dui. Nam fermentum sem lorem, eu condimentum ex bibendum a. Aliquam eu scelerisque magna, vitae feugiat orci. Nunc.
	
	Lorem ipsum dolor sit amet, consectetur adipiscing elit. Quisque lacus sem, ornare in commodo venenatis, pellentesque eget eros. Praesent at laoreet leo. Aliquam vitae elit eu tortor malesuada luctus at non lectus. Ut dapibus quis est nec facilisis. Phasellus et vehicula tellus. Suspendisse ut fringilla tortor. Aenean convallis hendrerit maximus. Ut aliquet purus in eros tincidunt, aliquet semper quam gravida. Donec nec augue quis eros malesuada scelerisque et non neque. Curabitur et orci consequat, vehicula neque vitae, facilisis mauris. Aliquam nec interdum dui. Nam fermentum sem lorem, eu condimentum ex bibendum a. Aliquam eu scelerisque magna, vitae feugiat orci. Nunc.
	
	Lorem ipsum dolor sit amet, consectetur adipiscing elit. Quisque lacus sem, ornare in commodo venenatis, pellentesque eget eros. Praesent at laoreet leo. Aliquam vitae elit eu tortor malesuada luctus at non lectus. Ut dapibus quis est nec facilisis. Phasellus et vehicula tellus. Suspendisse ut fringilla tortor. Aenean convallis hendrerit maximus. Ut aliquet purus in eros tincidunt, aliquet semper quam gravida. Donec nec augue quis eros malesuada scelerisque et non neque. Curabitur et orci consequat, vehicula neque vitae, facilisis mauris. Aliquam nec interdum dui. Nam fermentum sem lorem, eu condimentum ex bibendum a. Aliquam eu scelerisque magna, vitae feugiat orci. Nunc.
	
	
	\subsection{The Density Operator}
	\label{sec:densityop}
	We begin by summarising the Dirac formalism of quantum mechanics and motivating the need for a density operator approach to open systems. It is a fundamental postulate of quantum mechanics that the state vector, $\ket{\psi}$, provides a complete physical description of the system.  To any physical system, we associate a Hilbert space (the `state space'), which represents the entire set of states that the state vector can take. Operators are taken to act on elements of the Hilbert space, mapping each vector to some other vector within the space. Of particular importance are the `observable operators' which make precise our notion of measurement; every physically observable quantity has, associated with it, a linear Hermitian operator whose (real) eigenvalues represent possible outcomes of a measurement \cite{vonNeumann55}. We refer to `possible outcomes' to emphasise that, at its core, quantum mechanics is inherently probabilistic and measurements are typically described in terms of their expectation values.
	
	Of course, in the real world, it is rare to have complete knowledge of a system. Alongside quantum mechanical uncertainty, there may also be classical uncertainties relating to what state the system is actually assumed to be in. For instance, uncertainties in an experimental setup could be known to generate any of $n$ possible states, $\ket{\psi_n}$, each with a probability $p_n$. Our expectation of any measurement would then be a function of two unknowns: the inherent quantum mechanical uncertainty associated with any measurement of the state $\ket{\psi_n}$, as well as the classical uncertainty relating to which of the $n$ possible states the system is in at the moment of measurement. This `statistical mixture' of unknowns can be completely captured by introducing the density operator, $\hat{\rho}$, which we define \cite{Fano57}
	
	\begin{equation} \label{eq:1}
	\hat{\rho} = \sum_{j=1}^{n} p_j \ket{\psi_j}\bra{\psi_j}.
	\end{equation}
	
	The density operator represents our best description of the system and can, for all practical purposes, be regarded as the state of the system (in place of the state vector $\ket{\psi}$). To avoid confusion, we note at the offset that the density operator can, when equipped with a basis that is discrete and finite, be represented by a matrix and we shall use the terms density matrix and density operator interchangeably.
	
	From our definition \ref{eq:1}, it follows that the density matrix is both positive-semidefinite, 
	\begin{equation} \label{eq:2}
	\hat{\rho} \geq 0,
	\end{equation}
	and Hermitian,
	\begin{equation} \label{eq:3}
	\hat{\rho} = \hat{\rho}^\dagger .
	\end{equation}
	It can also be shown to have unit trace, 
	\begin{equation} \label{eq:4}
	\Tr[\hat{\rho}] = 1.
	\end{equation}
	
	As stated above, the main drive behind using a density operator approach stems from our desire to completely describe systems, accounting for all sources of uncertainty. It can be shown that the expectation value of any observable operator, $\hat{O}$, can be written as
	\begin{equation} \label{eq:5}
	\Tr[\hat{O}\hat{\rho}].
	\end{equation}
	
	This equation neatly captures all of the measurable/observable uncertainties of the system and, therefore, justifies our usage of density operators.
	
	\iffalse
	
	\subsection{Discrete Variable Systems}

	Our focus in this report is on CV systems. Before discussing those, however, it is worth briefly noting that discrete variable systems exist and arise frequently in quantum mechanics. As the name suggests, discrete variable systems are ones in which the possible outcomes of a measurement are discrete (or quantised). This manifests itself mathematically via operators whose eigenvalues are discrete. If, in addition to being discrete, the eigenvalues form a finite set, the system can be handled using matrix vector methods. There are numerous examples of discrete variable systems such as the energy levels of a quantum harmonic oscillator (infinite dimensional Hilbert space) or the spin of an electron (two dimensional Hilbert space).
	
	\fi
	
	\section{Continuous Variable Systems}
	\label{sec:cvs}
	
	\subsection{Definition}
	When eigenvalues (observable values) of a system have a continuous spectra, it naturally implies that the associated Hilbert space is infinite dimensional. Perhaps, the most obvious examples are the position or momentum of a particle. Noting that any measurement of, say, position must be an eigenvalue of the position operator it must follow that the state space is infinite in extent. 
	
	CV systems are quantum systems described by a pair of canonical operators, i.e. the pair are related via a Fourier transform. Position and momentum are one such example of a canonical/conjugate pair. Canonical pairs satisfy the canonical commutation relation which, for position and momentum, can be written \cite{Serafini05}
	\begin{equation} \label{eq:6}
	[\hat{x}, \hat{p}] = i\hbar\hat{\mathbb{I}}.
	\end{equation}
	
	One of the most fundamental examples of a CV system is the quantum harmonic oscillator which describes the dynamics of a particle in a quadratic potential. The Hamiltonian can be written as \cite{Adesso14, Braunstein}
	\begin{equation} \label{eq:7}
	\hat{H} = \frac{\hat{p}^2}{2m} + \frac{1}{2} m\omega^2\hat{x}^2,
	\end{equation}
	
	where $m$ is taken to be the mass of the particle and $\omega$, the angular frequency. For a system with $N$ non-interacting modes, the Hamiltonian can be written as a sum of the individual Hamiltonians 
	
	\begin{equation} \label{eq:8}
	\hat{H} = \sum_{k=1}^{N}\hat{H_k},\qquad\hat{H_k} =\frac{\hat{p_k}^2}{2m} + \frac{1}{2} m\omega_{k}^2\hat{x_k}^2.
	\end{equation}
	Here, $\hat{x_k}$ and $\hat{p_k}$ are canonical pairs satisfying the canonical commutation relation 
	\begin{equation} \label{eq:9}
	[\hat{x_j}, \hat{p_k}] = i\hbar \delta_{j\,k}\hat{\mathbb{I}} .
	\end{equation}
	It is convenient to group these operators into a single vector defined
	\begin{equation} \label{eq:10}
	\mathbf{\hat{r}} = (\hat{x_1},\hat{p_1},\hat{x_2},\hat{p_2},...,\hat{x_N},\hat{p_N})^T = (\hat{r}_1, \hat{r}_2, ..., \hat{r}_{2N})^T .
	\end{equation}
	From which the following commutation relations can be derived (using Eq.\ref{eq:6})
	\begin{equation*}
	[\hat{r}_1, \hat{r}_1] = [\hat{x_1}, \hat{x_2}] = 0,
	\end{equation*} 
	\begin{equation*}
	[\hat{r}_1, \hat{r}_2] = [\hat{x_1}, \hat{p_1}] = i,
	\end{equation*} 
	\begin{equation*}
	[\hat{r}_2, \hat{r}_1] = [\hat{p_1}, \hat{x_1}] = -i,
	\end{equation*} 
	\begin{equation*}
	[\hat{r}_2, \hat{r}_2] = [\hat{p_1}, \hat{p_2}] = 0,
	\end{equation*}
	which gives the matrix 
	\begin{equation*}
	\begin{pmatrix}
	0 & i  \\
	-i & 0 \\
	\end{pmatrix}.
	\end{equation*}
	Then the canonical commutation relation can be written as
	\begin{equation} \label{eq:11}
	[\hat{r_j}, \hat{r_k}] = i\Omega_{jk}\mathbb{\hat{I}},
	\end{equation}
	where $\Omega$ is a $2N \times 2N$, anti-symmetric matrix known as the symplectic form. It can be written as a matrix direct sum as follows
	\begin{equation} \label{eq:12}
	\Omega = \bigoplus_{j=1}^{N}\omega, \qquad\omega = 
	\begin{pmatrix}
	0 & 1  \\
	-1 & 0 \\
	\end{pmatrix}.
	\end{equation}
	
	The symplectic form is of considerable importance within CV systems, specifically within a set of states known as Gaussian states. The symplectic framework and Gaussian states are discussed in detail in \ref{sec:gaussian}. 
	
	\subsection{Phase Space Representation}
	
	As noted in Section \ref{sec:densityop}, the state of a quantum system can be represented using a density operator approach. Additionally, we can use $\hat{\rho}$ to build a phase space representation of the system where a real distribution is defined over the $2N$ dimensions in that space. 
	
	To transition between the representations, we must first introduce the  Weyl displacement operator, $\hat{D}_\textbf{r}$, \cite{Olivares12}
	\begin{equation} \label{eq:13}
	\hat{D}_\mathbf{r} = e^{i\mathbf{r}^{T}\Omega\mathbf{\hat{r}}}, 
	\end{equation}
	
	where $\mathbf{r} = (x_1, p_1, ..., x_N, p_N)^T $ represents a point in phase space and $\mathbf{\hat{r}}$ is the vector of canonical operators already introduced. As the name suggests, the displacement operator, $\hat{D}_\mathbf{r}$, shifts the state of the system by an amount $\mathbf{r}$ in phase space. The action of this is shown in Figure \ref{fig:RKPOINT}. 
	
	It can be shown that the set of all displacement operators, $\hat{D}_\mathbf{r}$, for all $\mathbf{r}$ within the phase space, form a basis in operator space, i.e. any operator can be represented as a linear combination of displacement operators. Given $\mathbf{r}$ varies continuously within the phase space, it is not surprising that the linear combination is represented via an integral; an arbitrary operator, $\hat{O}$, can be represented as a Fourier-Weyl transform \cite{Cahill68}
	\begin{equation} \label{eq:14}
	\hat{O} = \frac{1}{(2\pi)^n}\int \chi(\mathbf{r})\hat{D}_\textbf{r}  d\textbf{r} = \frac{1}{(2\pi)^n} \int \Tr[\hat{D}_\textbf{r}^\dagger\hat{O}]\hat{D}_\textbf{r}  d\textbf{r}, \qquad \hat{D}_\mathbf{r}^\dagger  = \hat{D}_\mathbf{-r}.
	\end{equation}  
	
	Here, $\chi(\mathbf{r})$ can be interpreted as a function that modulates the amplitude of $\hat{D}_\textbf{r}$. This equally well applies to an arbitrary quantum state, which can be written using a density operator
	\begin{equation} \label{eq:15}
	\hat{\rho} = \frac{1}{(2\pi)^n} \int \chi_\rho(\mathbf{r})\hat{D}_\textbf{r}  d\textbf{r} = \frac{1}{(2\pi)^n} \int \Tr[\hat{D}_\textbf{r}^\dagger \hat{\rho}]\hat{D}_\textbf{r}  d\textbf{r}.
	\end{equation} 
	
	\begin{figure} [h]
		\centering
		\includegraphics[scale = 1]{RKPOINT}
		\caption[Action of the displacement operator]{The displacement operator, $\hat{D}_\mathbf{r}$, shifts the state of the system in phase space.}
		\label{fig:RKPOINT}
	\end{figure}
	
	The action of this is shown in Figure \ref{fig:RKDENSITY}. We refer to $\chi_\rho$ as the characteristic function and, for every quantum state, $\hat{\rho}$, a corresponding characteristic function, $\chi_\rho$, exists \cite{Hillery84}. The characteristic function of the density operator is crucial to the definition of a Gaussian state, which is a key focus of this paper.
	
	\begin{figure} [h]
		\centering
		\includegraphics[scale = 1]{RKDENSITY}
		\caption[A Gaussian State representation]{The state of the system can be represented as a linear combination of displacement operators whose amplitude is modulated by the characteristic function.}
		\label{fig:RKDENSITY}
	\end{figure}
	
	
	\section{Gaussian States}
	\label{sec:gaussian} 
	\subsection{Definition and Interpretation}
	
	A Gaussian state is one in which the characteristic function is Gaussian. The utility of such a description comes about from the relatively few parameters required to characterise this state. A one dimensional Gaussian distribution is completely characterised by its first and second moments (the mean and variance) \cite{Ribeiro}. The generalisation to a $2N$ dimensional phase space is straightforward; the Gaussian state is completely characterised by a $2N$ dimensional vector of means and a $2N\times2N$ covariance matrix. The use of Gaussian states makes the handling of CV systems significantly more mathematically manageable.
	
	For $x \in \mathbb{R}^{2N}$, the standard Gaussian function can be written as \cite{Adesso14}
	\begin{equation} \label{eq:16}
	f(x) = Ae^{-\frac{1}{2}x^{T}Bx+ x^{T} y},
	\end{equation}
	where B is a $2N \times 2N$ matrix and $y$ is the mean vector. A quantum state $\hat{\rho}$ is Gaussian if and only if its characteristic function $\chi_\rho$ is a Gaussian function in phase space \cite{Schumaker86}, that is to say $\chi_\rho$ is of the form
	\begin{equation} \label{eq:17}
	\chi_G(\mathbf{r}) = e^{-\frac{1}{4}(\Omega^T\mathbf{r})^T\sigma(\Omega^T\mathbf{r}) - i(\Omega^T\mathbf{r})^T\mathbf{\bar{r}}},
	\end{equation} where $\sigma$ is taken to be the covariance matrix and $\mathbf{\bar{r}}$ is the vector of first moments. Written explicitly \cite{Adesso14},
	\begin{equation} \label{eq:18}
	\mathbf{\bar{r}} = (\langle\hat{x_1}\rangle, \langle\hat{p_1}\rangle, \langle\hat{x_2}\rangle, \langle\hat{p_2}\rangle, ..., \langle\hat{x_N}\rangle, \langle\hat{p_N}\rangle) = \Tr[\hat{\rho} \mathbf{\hat{r}}] ,
	\end{equation}
	and the covariance matrix has components,
	\begin{equation} \label{eq:19}
	\sigma_{jk} = \langle \mathbf{\hat{r}}_j\mathbf{\hat{r}}_k + \mathbf{\hat{r}}_k\mathbf{\hat{r}}_j \rangle - 2\langle \mathbf{\hat{r}}_j \rangle \langle \mathbf{\hat{r}}_k \rangle = \Tr[\hat{\rho} \{\mathbf{\hat{r_j}} - \mathbf{\bar{r}_j}, \mathbf{\hat{r}_k} - \mathbf{\bar{r}_k}\}].
	\end{equation}
	
	
	The covariance matrix is of central importance as it contains all the necessary information regarding properties such as purity and entanglement. It is an active area of research for many \cite{Adesso07, Plenio03}, and we often choose to isolate the matrix by taking the first moments to be zero ($\mathbf{\bar{r}} = 0$).
	
	Equivalently, Gaussian states can be interpreted as the ground and thermal states of systems with quadratic Hamiltonians \cite{Genoni16}. A quadratic Hamiltonian is defined as one whose position and momenta operators are, at most, quadratic in its definition. The $N$-mode quantum harmonic oscillator is an obvious example where both the momentum and position operators appear quadratically. They can thus be represented in matrix form as follows
	\begin{equation} \label{eq:20}
	\hat{H} = \frac{1}{2}\mathbf{\hat{r}}^{T}H\mathbf{\hat{r}}+\mathbf{\hat{r}}^{T}\mathbf{a}.
	\end{equation}
	
	
	A state $\hat{\rho}$ is Gaussian if and only if the Hamiltonian matrix $ \mathit{H} > 0$. Physically, this is reasonable as we are only considering stable systems (there must be a stable lower bound on $H$) and, mathematically, $H$ must be positive definite as $\sigma>0$. Therefore, we can equally well define a Gaussian state
	\begin{equation} \label{eq:21}
	\hat{\rho}_G = \frac{e^{-\beta \hat{H}}}{\Tr[e^{-\beta \hat{H}}]}.
	\end{equation}\\
	where $\beta = \frac{1}{k_BT}$ and the denominator is included to ensure that $\hat{\rho}_G$ has unit trace as per Eq.\ref{eq:4}. Assuming the linear terms do not contribute significantly to the quadratic Hamiltonian, $\hat{H}$, we can again isolate the quadratic matrix by dropping the linear terms completely \cite{Genoni16}, i.e.
	\begin{equation} \label{eq:22}
	\hat{H} = \frac{1}{2}\mathbf{\hat{r}}^{T}H\mathbf{\hat{r}}.
	\end{equation}
	
	Given the simplicity of this approach, Gaussian states hold a prominent position within many areas of research including optomechanics \cite{Genoni15} and atomic ensembles \cite{Sherson}. There have also been a number of reviews outlining the central role of Gaussian states within CV quantum information processing \cite{Napoli05, Weedbrook12}. 
	
	\subsection{Symplectic Framework}
	\label{sec:symplectic}
	Gaussian states are associated with the mathematical framework of the real symplectic group, $Sp(2N, \mathbb{R})$ \cite{Arvind95}. This group is characterised by the properties of symplectic transformations. A symplectic transformation is a unitary transformation which maps a Gaussian state to another Gaussian state within the phase space. The (linear) symplectic transformation maps the first moments according to
	\begin{equation}\label{eq:23}
	\mathbf{\bar{r}}' = S \mathbf{\bar{r}},
	\end{equation}
	and the second moments
	\begin{equation}\label{eq:24}
	\sigma' = S\sigma S^T,
	\end{equation}
	where S is a real, symplectic matrix \cite{Adesso14}.
	
	The set of all real symplectic matrices, $\{S\}$, then form a group within the real, $2N$ dimensional phase space called the symplectic group, $Sp(2N, \mathbb{R})$, that preserve the symplectic form $\Omega$ defined in Eq.\ref{eq:12}
	\begin{equation} \label{eq:25}
	\Omega = S\Omega S^T .
	\end{equation}
	
	For example, a squeezing transformation $S_r$, for $r \in \mathbb{R}$, is given as
	\begin{equation*}
	S_r = \begin{pmatrix}
	e^r & 0  \\
	0 & e^{-r} \\
	\end{pmatrix}.
	\end{equation*}
	
	Then, using Eq.\ref{eq:25} we can see the symplectic form is preserved
	\begin{equation*}
	S_r\Omega S_r^{T} = \begin{pmatrix}
	e^r & 0  \\
	0 & e^{-r} \\
	\end{pmatrix}
	\begin{pmatrix}
	0 & 1  \\
	-1 & 0 \\
	\end{pmatrix}
	\begin{pmatrix}
	e^r & 0  \\
	0 & e^{-r} \\
	\end{pmatrix}  = \begin{pmatrix}
	0 & e^re^{-r}  \\
	-e^{-r}e^r & 0 \\
	\end{pmatrix} = \begin{pmatrix}
	0 & 1 \\
	-1 & 0 \\
	\end{pmatrix} = \Omega .
	\end{equation*}
	
	Williamson's Theorem \cite{Williamson36} states that for every real, symmetric, positive-definite matrix  $\sigma$ there exists a symplectic transformation $S \in Sp(2N, \mathbb{R})$ that diagonalises $\sigma$
	\begin{equation} \label{eq:27}
	S\sigma S^T = \nu,
	\end{equation}
	where $\nu$ are the symplectic eigenvalues of $\sigma$ 
	\begin{equation} \label{eq:28}
	\nu = \bigoplus_{j=1}^{N} 
	\begin{pmatrix}
	\nu_j & 0  \\
	0 & \nu_j \\
	\end{pmatrix}.
	\end{equation}
	
	The set of symplectic eigenvalues of $\sigma$ form the symplectic spectrum, $\{\nu_j\}$. These symplectic eigenvalues can also be calculated by considering the (standard, non-symplectic) eigenvalues of $\lvert i\Omega \sigma \lvert$. Furthermore, using the Robertson-Schr{\"o}dinger uncertainty principle outlined in Section \ref{sec:uncertainty} it is possible to show that $\nu_j\geq 1$ for all $j = 1, ..., N$ \cite{Serafini05}.
	
	\subsection{The Uncertainty Principle}
	\label{sec:uncertainty}
	The covariance matrix, $\sigma$, is the covariance matrix of a quantum state if and only if it satisfies the uncertainty principle \cite{Simon94}. The Robertson-Schr{\"o}dinger uncertainty principle is given as \cite{Rob29, Sch30}
	\begin{equation} \label{eq:29}
	\sigma + i\Omega \geq 0.
	\end{equation} 
	We can show that the familiar Heisenberg uncertainty principle is a special case of the Robertson-Schr{\"o}dinger uncertainty principle. For $\mathbf{\hat{r}} = (\hat{x}, \hat{p})^T$, the covariance matrix is given as
	\begin{equation*}
	\sigma = 
	\begin{pmatrix}
	\sigma_{xx} & \sigma_{xp}  \\
	\sigma_{px} & \sigma_{pp} \\
	\end{pmatrix}
	= 
	\begin{pmatrix}
	2\Delta\hat{x}^2 & 0  \\
	0 & 2\Delta\hat{p}^2 \\
	\end{pmatrix} \qquad\text{where we assume $\sigma_{xp}=\sigma_{px}= \langle \hat{x}\hat{p} + \hat{p}\hat{x} \rangle - 2\bar{x}\bar{p} = 0$}.
	\end{equation*}
	From Eqs.\ref{eq:12} and \ref{eq:29} we can see
	\begin{equation*}
	\sigma + i\Omega = 
	\begin{pmatrix}
	2\Delta\hat{x}^2 & i  \\
	-i & 2\Delta\hat{p}^2 \\
	\end{pmatrix} \geq 0.
	\end{equation*}
	Taking the determinant gives
	\begin{equation*}
	4\Delta\hat{x}^{2}\Delta\hat{p}^2 - 1 \geq 0.
	\end{equation*}  
	This gives us Heisenberg's uncertainty principle ($\hbar = 1$) \cite{Heisenberg27} 
	\begin{equation*}
	\Delta\hat{x}^{2}\Delta\hat{p}^2 \geq \frac{1}{4} .
	\end{equation*}
	
	\section{Dynamics}
	\label{sec:dynamics}
	\subsection{The Heisenberg Picture}
	Till now, our discussion of the evolution of state has been described in terms of time independent operators acting upon a time varying state. This view of the world is described by the Schr{\"o}dinger picture in which all operators representing observables (Hamiltonian, position, time etc.) are constant in time. In contrast to this, however, the Heisenberg picture assumes the state of the system has no time dependency, rather it is acted upon by time dependent operators.
	
	Clearly, at $t=0$, both the Schr{\"o}dinger picture and the Heisenberg picture will coincide. Furthermore, the Heisenberg equation governs the dynamics of the system entirely analogous to the Schr{\"o}dinger equation \cite{Fujita}.
	
	
	\subsection{Closed System Dynamics}
	
	A closed system is one which experiences no influence from the external environment. The Heisenberg evolution equation for an operator $\hat{O}$ in a closed system is given by \cite{Fujita}
	\begin{equation*}
	\frac{d\hat{O}}{dt} = \frac{i}{\hbar}[\hat{H}, \hat{O}].
	\end{equation*}
	
	For any system, the vector of canonical operators $\mathbf{\hat{r}}$ evolves under the quadratic Hamiltonian, $\hat{H} = \frac{1}{2} H_{k l}\hat{r_k}\hat{r_l}$, according to \cite{Genoni16} 
	\begin{equation}\label{eq:31}
	\frac{d \mathbf{\hat{r}}}{dt} = \Omega H \mathbf{\hat{r}},
	\end{equation} which has the familiar solution
	\begin{equation}\label{eq:32}
	\mathbf{\hat{r}}(t) =e^{\Omega H t} \mathbf{\hat{r}}(0) = S\mathbf{\hat{r}}(0) ,
	\end{equation}
	where $S = e^{\Omega H t}$ is a real, symplectic matrix.
	
	\emph{Proof}
	\begin{flalign*}
	\frac{d\hat{r_j}}{dt} &= i[\frac{1}{2}H_{kl}\hat{r_k}\hat{r_l}, \hat{r_j}] = \frac{i}{2}H_{kl} [\hat{r_k}\hat{r_l}, \hat{r_j}]&\\
	&= \frac{i}{2} H_{kl}(\hat{r_k}[\hat{r_l}, \hat{r_j}] + [\hat{r_k}, \hat{r_j}]\hat{r_l})& \tag*{using $[\hat{AB}, \hat{C}] = \hat{A}[\hat{B},\hat{C}] + [\hat{A},\hat{C}]\hat{B}$}\\
	&= \frac{i}{2} H_{kl} (\hat{r_k}(i\Omega_{lj}) + (i\Omega_{kj})\hat{r_l})&\tag*{using Eq.\ref{eq:11}}\\
	&= -\frac{1}{2} H_{kl} (\hat{r_k}\Omega_{lj} + \Omega_{kj}\hat{r_l})&\\
	&= \frac{1}{2} H_{kl} (\hat{r_k}\Omega_{jl} + \Omega_{jk}\hat{r_l})& \tag*{using $\Omega_{jl} = - \Omega_{lj}$}\\
	&= \frac{1}{2} \Omega_{jk}H_{lk}\hat{r_k}  + \frac{1}{2} \Omega_{jk}H_{kl}\hat{r_l}&\\
	&= \Omega_{jl}H_{lk}\hat{r_k}& \tag*{using $H_{kl} = H_{lk}$.}&
	\end{flalign*}
	
	Hence, as required, we arrive at
	\begin{equation*}
	\frac{d \mathbf{\hat{r}}}{d t} = \Omega H \mathbf{\hat{r}} .
	\tag*{\rule{2mm}{2mm}}
	\end{equation*}
	
	
	
	The symplectic transformations, $S$, introduced in Eqs.\ref{eq:23} and \ref{eq:24} are equivalent to unitary transformations, $\hat{U} = e^{-i\hat{H}t}$, in the Hilbert space. Therefore, the symplectic transformation, $S$, preserves the symplectic form, $\Omega$, as outlined in Section \ref{sec:symplectic} \cite{Wang07}. 
	
	  
	\subsection{Open System Dynamics}
	Next, we turn our attention to open system dynamics, which will be the topic of this thesis. As outlined in \cite{Genoni16}, the covariance matrix, $\sigma$, fully characterises the shape of the Gaussian and, therefore, captures all the dynamical properties of interest. We would like to apply these techniques to a quantum system that is under the influence of its external environment. We make the simplifying assumption that the system is Markovian; the evolution of the state is entirely determined by the current state and has no dependence on the state's history. 
	
	The evolution of $\sigma$ is described by
	\begin{equation} \label{eq:33}
	\frac{d \sigma}{d t} = (\Omega H) \sigma + \sigma(\Omega H)^T  ,
	\end{equation}
	with the solution
	\begin{equation} \label{eq:34}
	\sigma(t) = e^{\Omega H t} \sigma(0) (e^{\Omega H t})^T = S\sigma(0)S^T .
	\end{equation}
	
	\emph{Proof}
	\begin{flalign*}
	\frac{d\sigma_{jk}}{dt} &= \frac{d}{dt}(\Tr[\hat{\rho} \{\mathbf{\hat{r_j}}, \mathbf{\hat{r_k}}\}])& \tag*{using Eq.\ref{eq:19} with no linear terms}\\
	&= \Tr[\hat{\rho} \frac{d}{dt}(\mathbf{\hat{r_j}}\mathbf{\hat{r_k}} + \mathbf{\hat{r_k}}\mathbf{\hat{r_j}})]& \tag*{\text{as $\hat{\rho}$ has no time dependence}}\\
	&= \Tr[\hat{\rho} (\frac{d \mathbf{\hat{r_j}}}{dt} \mathbf{\hat{r_k}} + \mathbf{\hat{r_j}}\frac{d \mathbf{\hat{r_k}}}{d t} +\frac{d \mathbf{\hat{r_k}}}{d t} \mathbf{\hat{r_j}} + \mathbf{\hat{r_k}}\frac{d\mathbf{\hat{r_j}}}{dt})]&\\
	&= \Tr[\hat{\rho} (\Omega_{jm}H_{mn}\mathbf{\hat{r_n}\hat{r_k}} + \mathbf{\hat{r_j}}\Omega_{km}H_{mn}\mathbf{\hat{r_n}}  + \Omega_{km}H_{mn}\mathbf{\hat{r_n}\hat{r_j}} + \mathbf{\hat{r_k}}\Omega_{jm}H_{mn}\mathbf{\hat{r_n}} )]&\tag*{\text{using Eq.\ref{eq:31}}}\\
	&= \Omega_{jm}H_{mn}\sigma_{nk} + \Omega_{km}H_{mn}\sigma_{jn}&\\
	&= (\Omega_{jm}H_{mn})\sigma_{nk} + \sigma_{jn}(\Omega H)_{nk}^T .&
	\end{flalign*}
	
	Hence, as required, we arrive at
	\begin{equation*}
	\frac{d \sigma}{d t} = (\Omega H) \sigma + \sigma(\Omega H)^T .  \tag*{\rule{2mm}{2mm}}
	\end{equation*}
	
	For a Gaussian open system, we can state a new equation of motion, the diffusion equation, \cite{Genoni16}
	\begin{equation} \label{eq:35}
	\frac{d \sigma}{d t} = A\sigma +\sigma A^{T} + D,
	\end{equation} where $A$ and $D$ are the drift and diffusion matrices respectively. For a coupling matrix, $C$, between the system and the environment, $A$ and $D$ are defined as
	\begin{equation} \label{eq:36}
	A = \Omega H_S + \frac{\Omega C\Omega C^T}{2}\quad\text{and}\quad D = \Omega C \sigma_{E} C^{T} \Omega^T ,
	\end{equation} where $H_S$ refers to the Hamiltonian of the system described by Eq.\ref{eq:22} and $\sigma_E$ is the environment covariance matrix.
	
	If we make the simplifying assumption that the system is interacting with a Markovian environment and further assume that this environment is in thermal equilibrium, then $A$ describes the dissipation in the thermal environment and $D$ describes the thermal noise \cite{Serafini17}. We note that $A$ and $D$ are drift and diffusion matrices of a quantum system if and only if \cite{Serafini17}
	
	\begin{equation} \label{eq:37}
	D + iA \Omega^T - i\Omega A^T \geq 0.
	\end{equation}
		
	In this thesis, the emphasis will be on the steady state diffusion equation obtained by setting Eq. \ref{eq:35} equal to zero,
	\begin{equation} \label{eq:38}
	0 = A\sigma +\sigma A^{T} + D.
	\end{equation}
	
	This equation describes a system in a relaxed state with unchanging characteristics. Steady state systems are appealing from a practical perspective; within dynamics, while most states can be readily prepared, they are typically hard to sustain but steady state systems are the easiest to do in this regard.
	
	
	\section{The Optimal Cooling Hamiltonian}
	
	\subsection{Motivation}
	The entropy of the system, $S$, is determined by the covariance matrix, $\sigma$, and the purity, $\gamma$, is given by $1-S$ \cite{Genoni16}. Our ultimate goal is to achieve low entropy, pure states as these are essential to the development of quantum technologies such as quantum computing \cite{Chuang}. To achieve a low entropy system, we use cooling -- a process that drains the entropy from a system by removing heat -- thus increasing the overall purity.
	
	The entropy of the state is determined by the determinant of $\sigma$ \cite{Genoni16}. Hence, the problem of finding the minimum entropy of a system is equivalent to finding a system that yields the smallest possible determinant. This investigation aims to find the minimum determinant of $\sigma$ by varying the Hamiltonian. For the sake of brevity, we shall refer to the minimum determinant of $\sigma$ as $\sigma_\text{min}$ throughout this report. Our motivation to vary the Hamiltonian stems from real-world considerations; in practice, it is hard to control (or minimise) the external factors that feed into the steady state diffusion equation (such as noise and dissipation) but it is feasible that we can control the form of the Hamiltonian. The optimal cooling Hamiltonian is therefore taken to be the one which, when considered within the definition of $A$ as shown in \ref{eq:36}, provides $\sigma_\text{min}$. 
	
	The work within this thesis is limited to quadratic Hamiltonians of the form described by \ref{eq:22}. Reducing the scope in this way is not a major limitation, however, as most noise (and other interaction terms) involve quadratic Hamiltonians and this formalism applies to a range of platforms including optomechanics, quantum light and trapped ions to name only a few \cite{Serafini17}. 
	
	\subsection{The Steady State Covariance Matrix}
	\label{sec:sigmasolve}
	
	To find the optimal cooling Hamiltonian, we must first solve the steady state diffusion equation to find an expression for the steady state covariance matrix, $\sigma$. We proceed to solve for $\sigma$ as follows
	\begin{align}\label{eq:40}
	0 &= A\sigma +\sigma A^{T} + D&\notag\\
	-D &= A\sigma +\sigma A^{T}&\notag\\
	\Vect{(-D)} &= \Vect{(A\sigma +\sigma A^{T})}&\notag\\
	\Vect{(-D)} &= \Vect{(A\sigma)} + \Vect{(\sigma A^{T})}&\notag\\
	\Vect{(-D)} &= (\mathbb{I} \otimes A)\Vect{(\sigma)} + (A \otimes \mathbb{I})\Vect{(\sigma)}\quad(*)\notag&\\
	-\Vect{(D)} &= (\mathbb{I} \otimes A + A \otimes \mathbb{I}) \Vect{(\sigma)}\notag&\\
	-(\mathbb{I} \otimes A + A \otimes \mathbb{I})^{-1}\Vect{(D)} &= \Vect{(\sigma)}.&
	\end{align}
	The covariance matrix, $\sigma$, can then be found by reshaping.
	Here, $\Vect(X)$ refers to the vectorisation of an $m \times n$ matrix X with
	\begin{equation*}
	\Vect(X)= (x_{11}, ..., x_{m1}, x_{12}, ..., x_{m2}, ..., x_{1n}, ..., x_{mn})^T ,
	\end{equation*}
	and line $(*)$ uses the fact that $\Vect{(AB)}=(\mathbb{I} \otimes A)\Vect{(B)}=(B^{T} \otimes \mathbb{I})\Vect{(A)}.$

	In this report, we address the issue of finding the optimal cooling Hamiltonian under various regimes (of increasing complexity). We first consider $\sigma_\text{min}$ in the one mode case where there is one degree of freedom such as a light mode. We then consider two variations on the minimum determinant of the top left $2 \times 2$ partition for $\sigma$ in the two mode case where we have an additional degree of freedom. For example within optomechanics, this additional mode would be a mechanical mode that is coupled to the light mode to form a combined system \cite{Aspelmeyer}. This has applications to the generation of macroscopic superposition \cite{Hoff} which, itself, is at the centre of many of the biggest debates exploring the foundational aspects of quantum mechanics and gravity \cite{Pikovski}, and therefore motivate this study.	
	
	\subsection{The One Mode Case}
	\label{sec:onemode}
	\subsubsection{Drift and Diffusion Matrices}
	\label{sec:onemodedefs}
	We will first consider the one mode case where $A$ and $D$ are taken to be the $2 \times 2$ matrices defined as follows
	\begin{align*}
	A &= A_0 + \Omega H \quad\text{where}\quad A_0 = -\frac{1}{2}\mathbb{I}_2,&\\
	D &= \chi \mathbb{I}_2.&
	\end{align*}
	In the definition of $A$, $A_0$ describes the dissipation towards the environment (the loss rate) and $\Omega H$ describes the unitary transformation where we restrict $H$ to $H>0$. In the definition of $D$, $\chi$ accounts for the noise (measured in frequency). In the one mode case, this is taken to be the only environment parameter. 
	
	$A$ and $D$ are drift and diffusion matrices of a quantum system and so the condition given in Eq. \ref{eq:37} holds. Substituting our expression for $A$, the condition becomes
	\begin{align} 
	&D + iA \Omega^T - i\Omega A^T \geq 0& \nonumber\\
	&D + iA_0 (-\Omega) + i (\Omega H) (-\Omega) - i\Omega A_{0}^{T} - i\Omega (-H\Omega) \geq 0& \nonumber\\
	&D - iA_0 \Omega - i\Omega A_0 \geq 0.& \label{eq:42}
	\end{align}
	
	Proceeding in matrix notation, \ref{eq:42} can be expanded	
	\begin{equation*}
	\begin{pmatrix}
	\chi & 0  \\
	0 & \chi \\
	\end{pmatrix} + i\begin{pmatrix}
	\frac{1}{2} & 0  \\
	0 & \frac{1}{2} \\
	\end{pmatrix}\begin{pmatrix}
	0 & 1  \\
	-1 & 0 \\
	\end{pmatrix} + i\begin{pmatrix}
	0 & 1  \\
	-1 & 0 \\
	\end{pmatrix}\begin{pmatrix}
	\frac{1}{2} & 0  \\
	0 & \frac{1}{2} \\
	\end{pmatrix}=
	\begin{pmatrix}
	\chi & i  \\
	-i & \chi \\
	\end{pmatrix} \geq 0.
	\end{equation*}

	Then, considering eigenvalues of the resulting matrix,
	\begin{align*}
	&(\chi- \lambda)(\chi- \lambda) - 1 = 0&\\
	&\lambda = \chi \pm 1,&
	\end{align*}
	we can see that the requirement for the matrix to be positive-semidefinite is satisfied if and only if $\chi \geq 1$. 
	
	Having solved for sigma using the approach outlined in Section \ref{sec:sigmasolve}, we can find $\sigma_\text{min}$ by varying the Hamiltonian, $H$, where the environment parameter, $\chi$, is fixed. For a physical system, we have the additional constraint that the Hamiltonian must be positive-definite. The approach taken in this report is to vary the Hamiltonian numerically in order to arrive at $\sigma_\text{min}$. To ensure that our search space is limited to Hamiltonians where $H>0$, we use the fact that any positive definite matrix can be factored via Cholesky decomposition. That is, $H$ can be written	$H = LL^T$,	where $L$ is a lower triangular matrix. We can then reverse-engineer this fact to generate arbitrary positive definite matrices by freely varying the components of $L$. To make this explicit, assuming $L$ is of the form \begin{equation}\label{eq:39}
	L = \begin{pmatrix}
	a & 0  \\
	b & c\\
	\end{pmatrix},
	\end{equation}
	it follows that the resulting Hamiltonian is given by
	\begin{equation} \label{eq:41}
	H = \begin{pmatrix}
	a^{2} & ab  \\
	ab & b^{2}+c^{2}\\
	\end{pmatrix}.
	\end{equation}
	This considerably simplifies the task of finding $\sigma_\text{min}$. What was formerly a minimisation problem over a $2\times2$ matrix subject to a constraint, has now been reduced to an unconstrained minimisation problem in $\mathbb{R}^3$ (over the three parameters $a$, $b$ and $c$). Naturally, this method scales to quadratic Hamiltonians of arbitrary size.
	
	\subsubsection{Method}

	A minimisation procedure was implemented in Python code to determine $\sigma_\text{min}$. For completeness, the code is available on GitHub \url{https://github.com/RiannaK/FinalReport}. The objective here was twofold: to understand the `shape' of the determinant (as a function of $a, b$ and $c$) and thus rule out the possibility that a local minimum was found; and to empirically derive a formula for $\sigma_\text{min}$ as a function of $\chi$.
	
	\subsubsection{Minimum Determinant}
	
	Numerical optimisation revealed that $\sigma_\text{min}$ is given by $\chi^{2}$. An analytical proof of this will be provided at the end of the section, however this relation was also numerically verified to 16 significant figures (machine precision) for the first 20 integers. A tabulated set of results is shown in Appendix \ref{sec:onemodetable}. A log-log plot of $\sigma_\text{min}$ against $\chi$ therefore yielded a gradient of 2 and an $R^2$ value of exactly 1, as shown in Figure \ref{fig:OneModePlot}.
	
	\begin{figure} [h]
		\centering
		\includegraphics[scale = 0.4]{OneModePlot2}
		\caption[One mode case: log-log plot of $\sigma_\text{min}$ against $\chi^2$]{Log-log plot of $\sigma_\text{min}$ against $\chi^2$ in the one-mode case.}
		\label{fig:OneModePlot}
	\end{figure}
	
	To be confident in our findings, however, it was necessary to rule out the possibility of the optimiser converging in a local minimum. Figure \ref{fig:OneModeGlobalMinima} shows the value of the determinant of $\sigma$ as a function of parameters $a, b$ and $c$. Visual inspection of the charts confirms that the determinant is lowest in the case that $a=c$ and $b=0$ (i.e. $H$ is some scaled form of the identity). Moreover, we can see that the value of the determinant monotonically increases away from $\sigma_\text{min}$ from which we can infer the absence of any local minima. 
	
	\begin{figure}
		\centering
		\includegraphics[scale = 0.5]{OneModeGlobalMinima}
		\caption[One mode case: determinant plots]{Plots showing the determinant of $\sigma$ as a function of the underlying parameters $a, b$ and $c$. The plots show that $\sigma$ is at a minimum for $a=c$ and $b=0$.}
		\label{fig:OneModeGlobalMinima}
	\end{figure}
	
	It is straightforward to see that this value for $\sigma$ is attainable in the trivial case that $H=0$. To see this, we substitute our equations for $A$ and $D$ into the steady state diffusion equation (Eq. \ref{eq:38}) and simplify
	

	\begin{align}	
	0 &= (-\frac{1}{2}\mathbb{I} + \Omega H)\sigma +\sigma (-\frac{1}{2}\mathbb{I} + \Omega H)^{T} + D&\nonumber\\
	0 &= -\sigma + (\Omega H)\sigma +\sigma (\Omega H)^{T} + D& \nonumber\\
	0 &= -\sigma + \Omega H\sigma +\sigma H\Omega^{T} + D \nonumber&\\
	0 &= -\sigma + \Omega H\sigma -\sigma H\Omega + \chi \mathbb{I}. &
	\end{align}
	
	Clearly, if $H=0$, we find $\sigma = \chi \mathbb{I}$ and thus a determinant of $\chi^2$. However, numerical optimisation also revealed that any Hamiltonian of the form $H=\alpha\mathbb{I}$ with $\alpha \geq0$ also resulted in a determinant of  $\chi^2$. This can be verified by substituting the stated form of the Hamiltonian into Eq. \ref{eq:40} but, justifying a posteriori, we trial a solution of the form $\sigma=\beta\mathbb{I}$ and demonstrate that it does, indeed, satisfy the steady state diffusion equation. 
	
	\begin{align*}
	0 &= -\sigma + \alpha (\Omega \sigma + \sigma \Omega^{T}) + \chi \mathbb{I}&\\
	0 &= -\beta \mathbb{I} + \alpha \beta (\Omega + \Omega^T) + \chi \mathbb{I}&\\
	0 & = -\beta \mathbb{I} + \chi \mathbb{I} & \tag*{as $\Omega$ is anti-symmetric}&\\
	\beta &= \chi.&
	\end{align*}
	
	Thus, $\sigma=\chi\mathbb{I}$ is a valid solution to the equation (when $H=\alpha\mathbb{I}$) and clearly has a determinant of $\chi^2$. We note that the trivial case of $H=0$ initially discussed is simply the trivial case of the more general solution $H=\alpha\mathbb{I}$ where $\alpha=0$.
	
	Of course, we could have argued that $\chi^2$ is the lowest determinant on physical grounds; the covariance matrix, $\sigma$, represents a physical state with $A$ and $D$ representing the drift and diffusion matrices of a quantum system respectively. Dividing though by $\chi$, we obtain
	\begin{equation*}
	0 = A \frac{\sigma}{\chi} + \frac{\sigma}{\chi} A^T + \mathbb{I}.
	\end{equation*}
	
	Here $\frac{\sigma}{\chi}$ is a steady state solution to the linear equation, and represents a physical state. We must therefore have that $\det\frac{\sigma}{\chi} \geq \mathbb{I}$ \cite{Adesso14}, resulting in $\det \sigma \geq \chi^2$. This provides an upper bound on $\sigma_\text{min}$ which can certainly be matched in the trivial case of $H=0$. We can, therefore, conclude that $\sigma_\text{min}(\chi)=\chi^{2}$. 

	
	\subsection{The Two Mode Case}
	\label{sec:twomode}
	\subsubsection{Drift and Diffusion Matrices}
	As noted, the methodology for investigating the one mode case can be readily scaled to larger problems. The two mode case is of considerable importance because it gives rise to non-trivial interactions between the two modes (such as entanglement), and can be applied to areas such as optomechanics. In the first instance, we generalise the drift and diffusion matrices to $4 \times 4$ matrices defined as follows
	\begin{align*}
	A &= A_0 + \Omega H \quad\text{where}\quad A_0 = -\frac{1}{2}\mathbb{I}_4\quad\text{and}\quad \Omega = \begin{pmatrix}
	0 & 1 & 0 & 0 \\
	-1 & 0 & 0 & 0\\
	0 & 0 & 0 & 1\\
	0 & 0 & -1 & 0\\
	\end{pmatrix}, &\\
	D &= \chi \mathbb{I}_4 .&
	\end{align*}
	
	Using the Cholesky decomposition procedure outlined in Section \ref{sec:onemode} we can construct an arbitrary positive-definite Hamiltonian, $H$, with $\frac{1}{2}n(n+1)=10$ degrees of freedom. We can then vary these parameters to find the minimum $2 \times 2$ top-left sub-determinant of the covariance matrix, $\sigma$. As was the case in Section \ref{sec:onemode}, we shall continue with our notation of using $\sigma_\text{min}$ and clarify that $\sigma_\text{min}$ refers to the minimum determinant of the first mode in an $n$-mode case (clearly this is simply the determinant in the one mode case). Again, assuming $A$ and $D$ are drift and diffusion matrices of a quantum system, Eq. \ref{eq:37} can be written as 

	\begin{align*}
	&\begin{pmatrix}
	\chi & 0 & 0 & 0\\
	0 & \chi & 0 & 0\\
	0 & 0 & \chi & 0 \\
	0 & 0 & 0 & \chi\\
	\end{pmatrix} + \frac{i}{2}\begin{pmatrix}
	1 & 0 & 0 & 0\\
	0 & 1 & 0 & 0\\
	0 & 0 & 1 & 0 \\
	0 & 0 & 0 & 1\\
	\end{pmatrix}\begin{pmatrix}
	0 & 1 & 0 & 0 \\
	-1 & 0 & 0 & 0\\
	0 & 0 & 0 & 1\\
	0 & 0 & -1 & 0\\
	\end{pmatrix} + \frac{i}{2}\begin{pmatrix}
	0 & 1 & 0 & 0 \\
	-1 & 0 & 0 & 0\\
	0 & 0 & 0 & 1\\
	0 & 0 & -1 & 0\\
	\end{pmatrix}\begin{pmatrix}
	1 & 0 & 0 & 0\\
	0 & 1 & 0 & 0\\
	0 & 0 & 1 & 0 \\
	0 & 0 & 0 & 1\\
	\end{pmatrix}&\\
	&=\begin{pmatrix}
	\chi & i & 0 & 0 \\
	-i & \chi & 0 & 0\\
	0 & 0 & \chi & i\\
	0 & 0 & -i & \chi\\
	\end{pmatrix}.
	\end{align*}
	
	Proceeding to evaluate the eigenvalues in an identical manner to the one mode case, we find
	\begin{align*}
    &((\chi-\lambda)^2 - 1)^2=0&\\
        &\lambda-\chi=\pm1&\\
	&\lambda = \chi \pm 1,&
	\end{align*}
	and thus $\chi \geq 1$ to ensure that the eigenvalues are positive.
	
	\subsubsection{Method}
	As was the process in the one mode case, a minimisation procedure was implemented in Python code to determine $\sigma_\text{min}$. To avoid repetition, we shall defer a lengthy discussion of the method to Section \ref{sec:twomodegeneralmethod} as the two mode case discussed here is simply a special case of the more generalised form to be discussed.
	
	\subsubsection{Minimum Sub-Determinant}
	\label{sec:twomodesimplemin}
	
	Our numerical analysis again revealed that the relation between $\sigma_\text{min}$ and $\chi$ is given by $\sigma_\text{min}(\chi)=\chi^{2}$. A tabulated set of results is shown in Appendix \ref{sec:twomodetablesimple} and a log-log plot of $\sigma_\text{min}$ against $\chi$ shows this relation in Figure \ref{fig:TwoModePlot}. It is noted that in some cases, the agreement between model and observation was only found accurate to 13 significant figures. While machine precision is generally accurate to 16, some floating point errors are inevitable in matrix operations and, in any case, the observed solution agrees with the model to within the tolerance of the optimiser.
	
	\begin{figure} [h]
		\centering
		\includegraphics[scale = 0.4]{OneModePlot2}
		\caption[Two mode case: log-log plot of $\sigma_\text{min}$ against $\chi^2$]{Log-log plot of $\sigma_\text{min}$ against $\chi^2$ in the two-mode case.}
		\label{fig:TwoModePlot}
	\end{figure}
	
	\subsubsection{Optimal Hamiltonian}
	
	Our numerical investigations to find $\sigma_\text{min}$ using random starting points for the optimisation resulted in two distinct forms for $H$ emerging, both of which yielded the same minimum, $\sigma_\text{min}(\chi)=\chi^{2}$. One solution is of the form
	
	\begin{equation}\label{eq:45}
		H = \begin{pmatrix}
			\alpha & \beta \\
			\beta & \gamma \\
		\end{pmatrix} \otimes \mathbb{I}_2,
	\end{equation}
	where $\alpha, \beta$ and $\gamma$ are arbitrary positive scalars (subject to $\alpha\gamma - \beta^2 > 0$ to guarantee $H>0$). The other solution was of the form

	\begin{equation}\label{eq:43}		
	H =
	\left[
	\begin{array}{c|c}
	\alpha \mathbb{I} & 0 \\
	\hline
	0 & \Gamma
	\end{array}
	\right],
	\end{equation}where $\alpha$ is an arbitrary positive scalar and $\Gamma$ is any positive definite matrix. It is interesting to note that these solutions are not completely unrelated as one can freely move from Eq. \ref{eq:45} to Eq. \ref{eq:43} by first smoothly varying $\beta \rightarrow 0$ and then by smoothly varying from $\gamma \mathbb{I}_2 \rightarrow \Gamma$.
	
	One can readily verify that these Hamiltonians give rise to $\sigma$ sub-determinant values of $\chi^2$. In the case of Eq. \ref{eq:45}, we substitute $\sigma=\chi\mathbb{I}_4$ into the diffusion equation,
	
	\begin{align*}
	\setlength\arraycolsep{3pt}
	&A\sigma +\sigma A^{T}&\\
	&= \begin{pmatrix}
	-\frac{1}{2} & \alpha & 0 & \beta\\
	-\alpha & -\frac{1}{2} & -\beta & 0 \\
	0 & \beta & -\frac{1}{2} & \gamma\\
	-\beta & 0 & -\gamma & -\frac{1}{2}\\
	\end{pmatrix}
	\begin{pmatrix}
	\chi & 0 & 0 & 0 \\
	0 & \chi & 0 & 0 \\
	0 & 0 & \chi & 0 \\
	0 & 0 & 0 & \chi \\
	\end{pmatrix} + \begin{pmatrix}
	\chi & 0 & 0 & 0 \\
	0 & \chi & 0 & 0 \\
	0 & 0 & \chi & 0 \\
	0 & 0 & 0 & \chi \\
	\end{pmatrix}\begin{pmatrix}
	-\frac{1}{2} & -\alpha & 0 & -\beta\\
	\alpha & -\frac{1}{2} & \beta & 0 \\
	0 & -\beta & -\frac{1}{2} & -\gamma\\
	\beta & 0 & \gamma & -\frac{1}{2}\\
	\end{pmatrix} \\
	&= 	\begin{pmatrix}
	-\chi & 0 & 0 & 0 \\
	0 & -\chi & 0 & 0 \\
	0 & 0 & -\chi & 0 \\
	0 & 0 & 0 & -\chi \\
	\end{pmatrix} = -D,
	\end{align*}
	with $\sigma$ clearly having a sub-determinant of $\chi^2$. Eq. \ref{eq:43} has a similar solution which we shall omit here. Instead, we note that the Hamiltonian is a block diagonal and so the top left $2\times2$ matrix in $\sigma$ must also be equal to $\chi\mathbb{I}_2$ by analogy to the solution shown above. Therefore, it will also have a a sub-determinant of $\chi^2$ as required.
	
	
 
	
	\subsection{Generalised Two Mode Cooling}
	\subsubsection{Drift and Diffusion Matrices}
	\label{sec:optodefs}	

	Having familiarised ourselves with the simple two mode case, we now turn our attention to a more general form for the diffusion and drift matrices. We modify the drift matrix to account for both phonon and photon interactions by defining $A = A_0 +\Omega H$ with
	\begin{equation*}
	A_0 = -\frac{1}{2} \begin{pmatrix}
	\kappa_1 & 0\\
	0 & \kappa_{2}\\
	\end{pmatrix} \otimes \mathbb{I}_2.
	\end{equation*} 
	We make a corresponding adjustment to the diffusion matrix by defining
	\begin{equation*}
	D = \begin{pmatrix}
	\chi_1 & 0\\
	0 & \chi_2\\
	\end{pmatrix} \otimes \mathbb{I}_2,
	\end{equation*}
	
	Before discussing the minimum determinant of $\sigma$ in this case, we must first find the constraints upon $\chi$ and $\kappa$ that ensure $A$ and $D$ satisfy the validity requirements imposed by Eq. \ref{eq:37}. Using the slightly simpler (but equivalent) condition Eq. \ref{eq:42}, 
	
	\begin{equation*}
		D - iA_0 \Omega - i\Omega A_0 \geq 0,
	\end{equation*}
	we substitute in our definitions of $A$ and $D$ and compute eigenvalues for the resultant matrix. 
	\begin{multline*}
	\begin{pmatrix}
	\chi_1 & 0 & 0 & 0\\
	0 & \chi_1 & 0 & 0\\
	0 & 0 & \chi_2 & 0 \\
	0 & 0 & 0 & \chi_2\\
	\end{pmatrix} + \frac{i}{2}\begin{pmatrix}
	\kappa_1 & 0 & 0 & 0\\
	0 & \kappa_1 & 0 & 0\\
	0 & 0 & \kappa_2 & 0 \\
	0 & 0 & 0 & \kappa_2\\
	\end{pmatrix}\begin{pmatrix}
	0 & 1 & 0 & 0 \\
	-1 & 0 & 0 & 0\\
	0 & 0 & 0 & 1\\
	0 & 0 & -1 & 0\\
	\end{pmatrix} \\
	+ \frac{i}{2}\begin{pmatrix}
	0 & 1 & 0 & 0 \\
	-1 & 0 & 0 & 0\\
	0 & 0 & 0 & 1\\
	0 & 0 & -1 & 0\\
	\end{pmatrix}\begin{pmatrix}
	\kappa_1 & 0 & 0 & 0\\
	0 & \kappa_1 & 0 & 0\\
	0 & 0 & \kappa_2 & 0 \\
	0 & 0 & 0 & \kappa_2\\
	\end{pmatrix}
	= \begin{pmatrix}
	\chi_1 & i\kappa_1 & 0 & 0 \\
	-i\kappa_1 & \chi_1 & 0 & 0\\
	0 & 0 & \chi_2 & i\kappa_2\\
	0 & 0 & -i\kappa_2 & \chi_2\\
	\end{pmatrix}.
	\end{multline*}
	
	Making use of standard determinant properties for block matrices, it is straightforward to compute the eigenvalues of this matrix.
	
	\begin{align*}
	&[(\chi_1- \lambda)^2 - \kappa_1^2][(\chi_2- \lambda)^2 - \kappa_2^2]=0&\\
	&\lambda=\chi_1\pm\kappa_1 \quad\text{and}\quad \lambda=\chi_2\pm\kappa_2 .&
	\end{align*}
	
	From this, we can see that Eq. \ref{eq:37} holds under the conditions that $\chi_1 \geq \kappa_1$ and $\chi_2 \geq \kappa_2$.
	
	
	\iffalse
	chi and kappa are, respectively, the noise (in units of frequency) and loss rates. `an assumption one needs to make for the cooling scenario to make sense: you will need to couple your system of interest (say, mechanics) with an auxiliary system (say, light) that will be asymptotically cooler, and that cools fast enough, otherwise the best is likely to be doing nothing' $\chi_1, \chi_2, \kappa_1, \kappa_2$? Check what ch1,2 kappa1,2 actually are.
	\fi
	
	\subsubsection{Method}
	\label{sec:twomodegeneralmethod}	
	
	The method undertaken to investigate the generalised two mode case was a natural extension of the methods employed in the previous sections. A $4\times4$ lower triangular matrix, $L$, was used to generate arbitrary positive definite matrices, $LL^T$, in the Hamiltonian. The number of degrees of freedom scales quadratically and so, rather than charting cross-sections of the $\sigma$ sub-determinant, a table of optimised determinants was produced from which a model could be inferred. The parameters $\chi_1, \chi_2, \kappa_1$ and $\kappa_2$ were fixed within runs but varied across runs.

	\subsubsection{Minimum Sub-Determinant}
	Our analysis showed that the minimum $2 \times 2$ top-left sub-determinant of the covariance matrix, $\sigma$, is given by
	
	\begin{equation}\label{eq:12345}
	\sigma_\text{min} = \Big(\dfrac{\chi_1 + \chi_2}{\kappa_1 + \kappa_2}\Big)^2.
	\end{equation}  
	We note that this is fully consistent with our findings in Section \ref{sec:twomodesimplemin} by substituting $\chi_1=\chi_2=\chi$ and $\kappa_1=\kappa_2=1$ into Eq. \ref{eq:12345}. From \ref{sec:optodefs} we know that 
	\begin{equation*}
	\chi \geq \kappa \implies	\Big(\dfrac{\chi_1 + \chi_2}{\kappa_1 + \kappa_2}\Big)^2 \geq 1,
	\end{equation*}
	as required for a physical system. 
	
	Eq. \ref{eq:12345} was numerically verified for every valid combination of integers up to 20 (8,855 permutations in total). The search space was made as large as was feasibly possible (given computation runtime constraints) in an attempt to identify possible discrepancies from the stated relation --- no such discrepancies were observed. A statistical $R^2$ test was performed to compare the modelled and observed data points and a value of 0.999999999992 was found indicating a very high degree of accuracy between model and actual. A (truncated) tabulated set of results is shown in Appendix \ref{sec:twomodetable}.
	
	\subsubsection{Optimal Hamiltonian}
	
	By inspecting the form of the Hamiltonians that emerged from the optimiser, it was possible to identify the optimal Hamiltonian in the general case. These findings are not without some caveats, however, which we must now address. First, we have not ruled out the possibility that there exists some other form of $H$ that also matches our lower bound. Second, we have not completely discarded the possibility that our lower bound is the true bound. Instead, we simply point to our empirical findings that over thousands of trials (using multiple starting points), every result has agreed with our model to within 12 significant figures which is within the tolerance of our optimiser.
	
	Clearly, in the case that $\chi_1=\chi_2$ and $\kappa_1=\kappa_2=1$, we must reconcile our solution with the ones found in Section \ref{sec:twomode}. Indeed, it is possible to show that solution \ref{eq:45} still holds for $\kappa_1=\kappa_2\neq1$. However, in the general case that the $\chi$ and $\kappa$ values can vary among each mode, the optimiser would only converge for very large Hamiltonians; the optimal form of $H$ suggested by the optimiser was given by,
	\begin{equation*}
	H = N \begin{pmatrix}
	1 & 1 \\
	1 & 1\\
	\end{pmatrix} \otimes \mathbb{I}_2,
	\end{equation*}
	for very large $N$. We can derive a (sub-optimal) solution for $\sigma$ for finite $N$ and then verify that this form converges on our modelled $\sigma_\text{min}$ expression in the limit $N \rightarrow \infty$.	The solution to the covariance matrix, $\sigma$, for finite $N$ is given by 
	
	\begin{equation} \label{eq:44}
	\sigma = \begin{pmatrix}
	a & 0 & 0 & c\\
	0 & a & -c & 0\\
	0 & -c & b & 0 \\
	c & 0 & 0 & b\\
	\end{pmatrix},
	\end{equation}	
	where, for convenience, we define $\alpha := \Big(\dfrac{\chi_1 + \chi_2}{\kappa_1 + \kappa_2}\Big)$ and $\beta := 1 + \frac{\kappa_1 \kappa_2}{4N^2}$ and then define
	
	
	\begin{align*}
	&a := \frac{\alpha + \frac{\chi_1 \kappa_2}{4N^2}}{\beta},&\\
	&b := \frac{\alpha + \frac{\chi_2 \kappa_1}{4N^2}}{\beta},&\\
	&c := \frac{\chi_1 \kappa_2 - \kappa_1 \chi_2}{2N\beta (\kappa_1+\kappa_2)}.&		
	\end{align*}


	It is straightforward to show that this expression for $\sigma$ satisfies the steady state diffusion equation

	\begin{align*}
	\setlength\arraycolsep{3pt}
	&A\sigma +\sigma A^{T}&\\
	&= \begin{pmatrix}
	-\frac{\kappa_1}{2} & N & 0 & N\\
	-N & -\frac{\kappa_1}{2} & -N & 0 \\
	0 & N & -\frac{\kappa_2}{2} & N\\
	-N & 0 & -N & -\frac{\kappa_2}{2}\\
	\end{pmatrix}\begin{pmatrix}
	a & 0 & 0 & c\\
	0 & a & -c & 0\\
	0 & -c & b & 0 \\
	c & 0 & 0 & b\\
	\end{pmatrix} + \begin{pmatrix}
	a & 0 & 0 & c\\
	0 & a & -c & 0\\
	0 & -c & b & 0 \\
	c & 0 & 0 & b\\
	\end{pmatrix}\begin{pmatrix}
	-\frac{\kappa_1}{2} & -N & 0 & -N\\
	N & -\frac{\kappa_1}{2} & N & 0 \\
	0 & -N & -\frac{\kappa_2}{2} & -N\\
	N & 0 & N & -\frac{\kappa_2}{2}\\
	\end{pmatrix}
	\end{align*}
	
	
	Commented out multlined for the moment as it's killing my pdf 
	\iffalse
	\[
	\begin{multlined}
	= \left(\begin{matrix}
	-\kappa_1 a - 2Nc & 0 \\
	0 & -\kappa_1 a - 2Nc \\ 
	0 & -\frac{1}{2}(\kappa_1 + \kappa_2)c - N(b-a)	\\ 
	\frac{1}{2}(\kappa_1 + \kappa_2)c - N(b-a) & 0 \\ 
	\end{matrix}\right. \quad\dotsm  \\
	%second part of matrices ...
	\dotsm\quad\left.\begin{matrix}
	0 & \frac{1}{2}(\kappa_1 + \kappa_2)c - N(b-a)     \\
	-\frac{1}{2}(\kappa_1 + \kappa_2)c - N(b-a) & 0     \\
	-\kappa_2 b + 2Nc & 0     \\
	0 & -\kappa_2 b + 2Nc		\\
	\end{matrix}\right)
	\end{multlined}
	\]   
	\fi
	
	\begin{align*}
	&= -D.&	
	\end{align*}
	
	This is verified in Appendix \ref{sec:verifyappen}. In limit that $N \rightarrow \infty$ we first note that $\beta \rightarrow 1$. Then, inspecting terms $a$ and $b$ we see that $a \rightarrow \alpha$, $b \rightarrow \alpha$ and the determinant therefore converges on the minimum determinant $\alpha^2=\Big(\dfrac{\chi_1 + \chi_2}{\kappa_1 + \kappa_2}\Big)^2$.
	
	
	\section{Conclusion}
	\label{sec:conc}
	
	We have 

	\begin{appendices}
	
	\section{Optimisation Results} 
	\subsection{The One Mode Case}
	\label{sec:onemodetable}
	\begin{center}
		\begin{tabular}{c|cc|c}
			$\chi$ & Result & Model & \% Agreement \\
			\hline
			1 & 1.0000000000000004 & 1.0000 & 100.0000\% \\
			2 & 4.0000000000000062 & 4.0000 & 100.0000\% \\
			3 & 9.0000000000000462 & 9.0000 & 100.0000\% \\
			4 & 16.0000000000000142 & 16.0000 & 100.0000\% \\
			5 & 25.0000000000000178 & 25.0000 & 100.0000\% \\
			6 & 36.0000000000000142 & 36.0000 & 100.0000\% \\
			7 & 49.0000000000000142 & 49.0000 & 100.0000\%\\
			8 & 64.0000000000002558 & 64.0000 & 100.0000\% \\
			9 & 80.9999999999999574 & 81.0000 & 100.0000\% \\
			10 & 100.0000000000000426 & 100.0000 & 100.0000\% \\
			11 & 121.0000000000008953 & 121.0000 & 100.0000\% \\
			12 & 144.0000000000006537 & 144.0000 & 100.0000\% \\
			13 & 169.0000000000001421 & 169.0000 & 100.0000\% \\
			14 & 196.0000000000004547 & 196.0000 & 100.0000\% \\
			15 & 225.0000000000001990 & 225.0000 & 100.0000\% \\
			16 & 256.0000000000001705 & 256.0000 & 100.0000\% \\
			17 & 289.0000000000015916 & 289.0000 & 100.0000\% \\
			18 & 324.0000000000001705 & 324.0000 & 100.0000\% \\
			19 & 360.9999999999998295 & 361.0000 & 100.0000\% \\
			20 & 400.0000000000002274 & 400.0000 & 100.0000\% \\
			\hline
		\end{tabular}
	\end{center}

\subsection{The Two Mode Case}
\label{sec:twomodetablesimple}
\begin{center}
	\begin{tabular}{c|cc|c}
		$\chi$ & Result & Model & \% Agreement \\
		\hline
		1 & 1.0000000000000067 & 1.0000 & 100.0000\% \\
		2 & 4.000000000000049 & 4.0000 & 100.0000\% \\
		3 & 9.000000000000318 & 9.0000 & 100.0000\% \\
		4 & 16.000000000002693 & 16.0000 & 100.0000\% \\
		5 & 25.00000000000709 & 25.0000 & 100.0000\% \\
		6 & 36.000000000002075 & 36.0000 & 100.0000\% \\
		7 & 49.000000000003716 & 49.0000 & 100.0000\% \\
		8 & 64.00000000001924 & 64.0000 & 100.0000\% \\
		9 & 81.00000000002686 & 81.0000 & 100.0000\% \\
		10 & 100.00000000000111 & 100.0000 & 100.0000\% \\
		11 & 121.00000000001218 & 121.0000 & 100.0000\% \\
		12 & 144.00000000003658 & 144.0000 & 100.0000\% \\
		13 & 169.000000000006 & 169.0000 & 100.0000\% \\
		14 & 196.0000000002069 & 196.0000 & 100.0000\% \\
		15 & 225.00000000006136 & 225.0000 & 100.0000\% \\
		16 & 256.00000000002154 & 256.0000 & 100.0000\% \\
		17 & 289.00000000010783 & 289.0000 & 100.0000\% \\
		18 & 324.00000000078063 & 324.0000 & 100.0000\% \\
		19 & 361.0000000000396 & 361.0000 & 100.0000\% \\
		20 & 400.0000000003861 & 400.0000 & 100.0000\% \\
		\hline
	\end{tabular}
\end{center}

	\subsection{The General Two Mode Case}
	\label{sec:twomodetable}
		\begin{center}
		\begin{tabular}{cccc|cc|c}
			$\chi_1$ & $\chi_2$ & $\kappa_1$ & $\kappa_2$ & Result & Model & \% Agreement \\
			\hline
			1 & 1 & 1 & 1 & 1.0000 & 1.0000 & 100.0000\% \\
			2 & 1 & 1 & 1 & 2.2500 & 2.2500 & 100.0001\% \\
			2 & 2 & 1 & 1 & 4.0000 & 4.0000 & 100.0000\% \\
			2 & 2 & 1 & 2 & 1.7778 & 1.7778 & 100.0001\% \\
			2 & 2 & 2 & 2 & 1.0000 & 1.0000 & 100.0000\% \\
			3 & 1 & 1 & 1 & 4.0000 & 4.0000 & 100.0002\% \\
			3 & 2 & 1 & 1 & 6.2500 & 6.2500 & 100.0001\% \\
			3 & 2 & 1 & 2 & 2.7778 & 2.7778 & 100.0002\% \\
			3 & 2 & 2 & 2 & 1.5625 & 1.5625 & 100.0003\% \\
			3 & 3 & 1 & 1 & 9.0000 & 9.0000 & 100.0000\% \\
			3 & 3 & 1 & 2 & 4.0000 & 4.0000 & 100.0002\% \\
			3 & 3 & 2 & 2 & 2.2500 & 2.2500 & 100.0000\% \\
			3 & 3 & 1 & 3 & 2.2500 & 2.2500 & 100.0004\% \\
			3 & 3 & 2 & 3 & 1.4400 & 1.4400 & 100.0001\% \\
			3 & 3 & 3 & 3 & 1.0000 & 1.0000 & 100.0000\% \\
			4 & 1 & 1 & 1 & 6.2500 & 6.2500 & 100.0005\% \\
			4 & 2 & 1 & 1 & 9.0000 & 9.0000 & 100.0001\% \\
			4 & 2 & 1 & 2 & 4.0000 & 4.0000 & 100.0001\% \\
			4 & 2 & 2 & 2 & 2.2500 & 2.2500 & 100.0001\% \\
			4 & 3 & 1 & 1 & 12.2500 & 12.2500 & 100.0000\% \\
			4 & 3 & 1 & 2 & 5.4444 & 5.4444 & 100.0001\% \\
			4 & 3 & 2 & 2 & 3.0625 & 3.0625 & 100.0004\% \\
			4 & 3 & 1 & 3 & 3.0625 & 3.0625 & 100.0000\% \\
			4 & 3 & 2 & 3 & 1.9600 & 1.9600 & 100.0002\% \\
			4 & 3 & 3 & 3 & 1.3611 & 1.3611 & 100.0002\% \\
			4 & 4 & 1 & 1 & 16.0000 & 16.0000 & 100.0000\% \\
			4 & 4 & 1 & 2 & 7.1111 & 7.1111 & 100.0002\% \\
			4 & 4 & 2 & 2 & 4.0000 & 4.0000 & 100.0000\% \\
			4 & 4 & 1 & 3 & 4.0000 & 4.0000 & 100.0003\% \\
			4 & 4 & 2 & 3 & 2.5600 & 2.5600 & 100.0002\% \\
			4 & 4 & 3 & 3 & 1.7778 & 1.7778 & 100.0000\% \\
			4 & 4 & 1 & 4 & 2.5600 & 2.5600 & 100.0002\% \\
			4 & 4 & 2 & 4 & 1.7778 & 1.7778 & 100.0001\% \\
			4 & 4 & 3 & 4 & 1.3061 & 1.3061 & 100.0001\% \\
			4 & 4 & 4 & 4 & 1.0000 & 1.0000 & 100.0000\% \\
			\hline
		\end{tabular}
	\end{center}
		\section{The Two Mode Cooling General Solution}
	\label{sec:verifyappen}
	
	We can verify that Eq. \ref{eq:44} satisfies the steady state diffusion equation with drift and diffusion matrices as described in \ref{sec:optodefs}.
	
	To verify this, first we show that the anti-diagonal terms are zero. Evaluating $\frac{1}{2}(\kappa_1 + \kappa_2)c$ and $N(b-a)$ separately
	\begin{align*}
	&\frac{1}{2}(\kappa_1 + \kappa_2) c = \frac{1}{2}(\kappa_1 + \kappa_2) \cdot \frac{\chi_1 \kappa_2 - \kappa_1 \chi_2}{2N\beta (\kappa_1+\kappa_2)} = \frac{1}{4N\beta}(\chi_1 \kappa_2 - \kappa_1 \chi_2),&\\
	&N(b-a)= N \Big(\frac{\alpha + \frac{\chi_2 \kappa_1}{4N^2}}{\beta} -\frac{\alpha + \frac{\chi_1 \kappa_2}{4N^2}}{\beta}\Big) = \frac{1}{4N\beta}(\chi_2 \kappa_1 - \chi_1 \kappa_2),&\\
	&\text{gives}\quad\frac{1}{2}(\kappa_1 + \kappa_2)c + N(b-a) = 0\text{ as required.}&
	\end{align*}
	
	Then, evaluating the diagonal terms $-\kappa_1 a - 2Nc$ and $-\kappa_2 b + 2Nc$
	
	\begin{align*}
	-\kappa_1 a - 2Nc &= \frac{-\kappa_1}{\beta} \Big(\alpha + \frac{\chi_1 \kappa_2}{4N^2} \Big) - \frac{\chi_1 \kappa_2 - \kappa_1 \chi_2}{\beta (\kappa_1+\kappa_2)}&\\
	&= \frac{-\kappa_1 (\kappa_1+\kappa_2) \Big(\alpha + \frac{\chi_1 \kappa_2}{4N^2} \Big) - \chi_1 \kappa_2 - \kappa_1 \chi_2}{\beta (\kappa_1+\kappa_2)}&\\
	&= \frac{-\chi_1(\kappa_1+\kappa_2)- (\kappa_1+\kappa_2)\chi_1 \frac{\kappa_1 \kappa_2}{4N^2}}{\beta (\kappa_1+\kappa_2)}&\\
	&= -\chi_1 \frac{\beta}{\beta}&\\
	&= -\chi_1&
	\end{align*}
	
	\begin{align*}
	-\kappa_2 b + 2Nc &= \frac{-\kappa_2}{\beta} \Big(\alpha + \frac{\chi_2 \kappa_1}{4N^2} \Big) + \frac{\chi_1 \kappa_2 - \kappa_1 \chi_2}{\beta (\kappa_1+\kappa_2)}&\\
	&= \frac{-\kappa_2 (\kappa_1+\kappa_2) \Big(\alpha + \frac{\chi_2 \kappa_1}{4N^2} \Big) + \chi_1 \kappa_2 - \kappa_1 \chi_2}{\beta (\kappa_1+\kappa_2)}&\\
	&= \frac{-\chi_2(\kappa_1+\kappa_2)- (\kappa_1+\kappa_2)\chi_2 \frac{\kappa_1 \kappa_2}{4N^2}}{\beta (\kappa_1+\kappa_2)}&\\
	&= -\chi_2 \frac{\beta}{\beta}&\\
	&= - \chi_2&
	\end{align*}

	
	\end{appendices}
	
	\newpage
	\bibliography{bibl_file}
	\bibliographystyle{ieeetr}
	
	
\end{document} 